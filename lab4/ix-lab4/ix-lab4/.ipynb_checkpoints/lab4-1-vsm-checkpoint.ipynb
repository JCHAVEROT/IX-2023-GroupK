{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *K*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Mathieu Sauser*\n",
    "* *Luca Mouchel*\n",
    "* *Jérémy Chaverot*\n",
    "* *Heikel Jebali*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jchavero/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jchavero/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "from utils import load_json, load_pkl\n",
    "import json\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps each term to the number of times it appears\n",
    "freqs = {}\n",
    "# Map each course (cours['courseId']) to its processed description, i.e a list of words or n-grams\n",
    "processedCourses = {}\n",
    "\n",
    "for i, course in enumerate(courses):\n",
    "    description = course['description']\n",
    "    description = [char.lower() for char in description]\n",
    "    description = ''.join(description)\n",
    "\n",
    "    # Remove punctuation marks\n",
    "    description = re.sub(r'[^\\w\\s]', '', description)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(description)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords]                        \n",
    "        \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizedTokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "    # Get the frequency of each token and update the freqs map\n",
    "    freqDist = FreqDist(lemmatizedTokens)\n",
    "    for token in lemmatizedTokens:\n",
    "        if token not in freqs.keys():\n",
    "            freqs[token] = freqDist[token]\n",
    "        else:\n",
    "            freqs[token] += freqDist[token]\n",
    "\n",
    "    # Add n-grams to the vocabulary\n",
    "    nGramRange = (2, 3)  # Specify the range of n-grams to consider\n",
    "    \n",
    "    ngrams = []\n",
    "    for i in range(nGramRange[0], nGramRange[1] + 1):\n",
    "        ngrams.extend(list(nltk.ngrams(lemmatizedTokens, i)))\n",
    "        \n",
    "    ngrams = [' '.join(ngram) for ngram in ngrams]\n",
    "    for ngram in ngrams:\n",
    "        if ngram not in freqs.keys():\n",
    "            freqs[ngram] = 1\n",
    "        else:\n",
    "            freqs[ngram] += 1\n",
    "            \n",
    "    vocabulary = lemmatizedTokens + ngrams\n",
    "    \n",
    "    processedCourses[course['courseId']] = vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**student** is the most used (lemmatized) word, with 9887 apparitions\n"
     ]
    }
   ],
   "source": [
    "# Find most frequent token\n",
    "mostFreq = float('-inf')\n",
    "mostFreqToken = ''\n",
    "for token, freq in freqs.items():\n",
    "    if freq > mostFreq: \n",
    "        mostFreq = freq\n",
    "        mostFreqToken = token\n",
    "\n",
    "print(f'**{mostFreqToken}** is the most used (lemmatized) word, with {mostFreq} apparitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words: {'student', 'method', 'system'}\n"
     ]
    }
   ],
   "source": [
    "freqWords = set([word for word in freqs.keys() if freqs[word] > mostFreq * 0.6])\n",
    "infreqWords = set([word for word in freqs.keys() if freqs[word] < 3])\n",
    "\n",
    "print(f'Most frequent words: {freqWords}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '20 midterm', '30', '30 final', '30 final exam', '50', 'acquired', 'activity', 'activity lecture', 'ad', 'ad', 'algebra', 'algebra', 'algorithm', 'algorithm', 'algorithm data', 'algorithm data structure', 'analysis', 'analytics', 'analytics', 'application', 'application', 'assessment', 'assessment method', 'assessment method project', 'auction', 'auction', 'balance', 'based', 'based', 'basic', 'basic', 'basic', 'basic linear', 'basic linear algebra', 'basic material', 'basic model', 'cathedra', 'chain', 'class', 'class', 'class', 'cloud', 'clustering', 'clustering', 'collection', 'com300', 'combination', 'communication', 'communication com300', 'community', 'community', 'computing', 'computing', 'concept', 'concept', 'concept start', 'concrete', 'content', 'content class', 'course', 'course', 'course basic', 'course stochastic', 'course stochastic model', 'coverage', 'current', 'data', 'data', 'data', 'data', 'data', 'data', 'data mining', 'data mining', 'data mining', 'data structure', 'datasets', 'datasets', 'decade', 'dedicated', 'designed', 'detection', 'detection', 'dimensionality', 'dimensionality reduction', 'draw', 'ecommerce', 'ecommerce', 'effectiveness', 'efficiency', 'end', 'end student', 'end student explore', 'exam', 'exam 50', 'expected', 'expected student', 'expected student activity', 'explore', 'explore', 'explore', 'explore', 'explores', 'field', 'final', 'final exam', 'final exam 50', 'framework', 'function', 'fundamental', 'fundamental concept', 'good', 'graph', 'graph', 'graph theory', 'hadoop', 'hadoop', 'handson', 'homework', 'homework', 'important', 'important concept', 'important concept start', 'information', 'information', 'information retrieval', 'infrastructure', 'internet', 'internet', 'java', 'key', 'keywords', 'keywords data', 'knowledge', 'knowledge acquired', 'lab', 'lab', 'lab', 'lab session', 'laboratory', 'laboratory session', 'largescale', 'largescale', 'largescale', 'learning', 'learning', 'learning', 'learning', 'learning outcome', 'learning outcome end', 'learning prerequisite', 'learning prerequisite required', 'learning technique', 'lecture', 'lecture', 'lecture homework', 'linear', 'linear', 'linear algebra', 'linear algebra', 'machine', 'machine', 'machine learning', 'machine learning', 'machine learning technique', 'main', 'mapreduce', 'markov', 'markov chain', 'material', 'material', 'medium', 'method', 'method', 'method cathedra', 'method project', 'midterm', 'mining', 'mining', 'mining', 'model', 'model', 'model', 'model', 'model', 'model communication', 'model communication com300', 'modeling', 'modeling analysis', 'network', 'networking', 'networking', 'networking', 'number', 'number', 'online', 'online', 'online', 'online', 'online', 'outcome', 'outcome end', 'outcome end student', 'past', 'practical', 'practical question', 'practice', 'prerequisite', 'prerequisite required', 'prerequisite required course', 'problem', 'problem', 'problem teaching', 'problem teaching method', 'project', 'provide', 'question', 'real', 'real world', 'realworld', 'realworld', 'realworld', 'realworld', 'realworld application', 'recommended', 'recommended course', 'recommended course basic', 'recommender', 'recommender', 'recommender system', 'recommender system', 'reduction', 'related', 'related field', 'required', 'required course', 'required course stochastic', 'retrieval', 'search', 'service', 'service', 'service', 'session', 'session', 'session expected', 'session expected student', 'social', 'social', 'social', 'social', 'social', 'social medium', 'specifically', 'start', 'statistic', 'stochastic', 'stochastic model', 'stochastic model communication', 'stream', 'stream', 'structure', 'student activity', 'student activity lecture', 'student explore', 'system', 'system', 'teaching', 'teaching method', 'teaching method cathedra', 'technique', 'theoretical', 'theory', 'topic', 'typical', 'ubiquitous', 'user', 'weekly', 'work', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Remove frequent and infrequent words and modify the freqs and processedCourses map accordingly\n",
    "temp = {}\n",
    "for term, freq in freqs.items():\n",
    "    if freq < mostFreq and freq > 3:\n",
    "        temp[term] = freq\n",
    "        \n",
    "freqs = temp\n",
    "\n",
    "newProcessedCourses = {}\n",
    "for courseId, description in processedCourses.items():\n",
    "    newProcessedCourses[courseId] = [word for word in description if word in freqs.keys()]\n",
    "    \n",
    "    if courseId == 'COM-308':\n",
    "        print(sorted(newProcessedCourses['COM-308']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pre-processed courses.txt file to disk\n",
    "def get_preprocessed_courses(data): \n",
    "    with open('data/preprocessed_courses.txt', 'w') as f:\n",
    "        for i in range(len(courses)):\n",
    "            c = data[i]\n",
    "            entry = {'courseId': c['courseId'],\n",
    "                     'name': c['name'],\n",
    "                     'description': newProcessedCourses[c['courseId']]}\n",
    "            json.dump(entry, f)\n",
    "            if i < len(courses) - 1: f.write(',\\n')\n",
    "            \n",
    "preprocessed_courses = get_preprocessed_courses(courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We removed the stopwords because they don't carry a lot of information for the meaning of the whole text and it will reduce the size of the TF-IDF matrix.\n",
    "- We removed the punctuation for similar reasons.\n",
    "- We removed very frequent words because they don't help us differentiate between documents if they appear everywhere\n",
    "- We removed infrequent words because they don't represent the meaning of the text if they are not used often\n",
    "- We lemmatized the words to group words with very similar meaning (e.g. 'chain' and chains')\n",
    "- We added 2-grams and 3-grams to have additional information on how the words group together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps each term to an index\n",
    "termToIdx = {}\n",
    "for i, term in enumerate(freqs.keys()):\n",
    "    termToIdx[term] = i\n",
    "\n",
    "# Maps each document to an index\n",
    "docToIdx = {}\n",
    "j = 0\n",
    "for courseId, description in newProcessedCourses.items():\n",
    "    docToIdx[courseId] = j\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(termToIdx.keys())\n",
    "N = len(docToIdx.keys())\n",
    "TD = np.zeros((M, N))\n",
    "\n",
    "# Create the term-document matrix\n",
    "for courseId, description in newProcessedCourses.items():\n",
    "    for term in description:\n",
    "        TD[termToIdx[term]][docToIdx[courseId]] += 1\n",
    "\n",
    "# Get the number of terms for each document\n",
    "termsPerDoc = np.sum(TD, axis=0)\n",
    "# Compute TF and IDF to get TFIDF\n",
    "TF = TD / termsPerDoc\n",
    "IDF = np.log2(N / np.count_nonzero(TD, axis=1))\n",
    "TFIDF = np.transpose(np.transpose(TF) * IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./TFIDF.npy', TFIDF)\n",
    "np.save('./termToIdx.npy', termToIdx)\n",
    "np.save('./docToIdx.npy', docToIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Top 15 terms with highest TF-IDF scores in IX course ------\n",
      "\n",
      "------------------\n",
      "Term: Score\n",
      "------------------\n",
      "online: 0.0880\n",
      "realworld: 0.0879\n",
      "social: 0.0823\n",
      "data mining: 0.0795\n",
      "explore: 0.0764\n",
      "mining: 0.0722\n",
      "networking: 0.0687\n",
      "hadoop: 0.0624\n",
      "largescale: 0.0615\n",
      "recommender: 0.0582\n",
      "ecommerce: 0.0582\n",
      "recommender system: 0.0582\n",
      "service: 0.0559\n",
      "auction: 0.0553\n",
      "datasets: 0.0530\n"
     ]
    }
   ],
   "source": [
    "ixScores = np.argsort(-TFIDF[:, docToIdx['COM-308']])\n",
    "top15idx = ixScores[:15]\n",
    "\n",
    "print('------ Top 15 terms with highest TF-IDF scores in IX course ------\\n')\n",
    "print('------------------')\n",
    "print('Term: Score')\n",
    "print('------------------')\n",
    "for idx in top15idx:\n",
    "    for term, idx2 in termToIdx.items():\n",
    "        if idx == idx2:\n",
    "            print(f'{term}: {TFIDF[idx, docToIdx[\"COM-308\"]]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high score means that the term appears a lot in the document but not that much in the rest of the rest of the corpus, meaning that it is really relevant for this given document.\n",
    "\n",
    "A low score means that either the term is infrequent in the document and frequent everywhere, or infrequent everywhere, or very frequent everywhere hence reducing its relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Reverse the map of documents to index\n",
    "idxToDoc = {v: k for k, v in docToIdx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(i, j):\n",
    "    '''\n",
    "    Computes the similarity between two vectors using cosine distance\n",
    "    \n",
    "    Parameters:\n",
    "        i (int): The index of the i-th document\n",
    "        j (int): The index of the j-th document\n",
    "        \n",
    "    Returns:\n",
    "        The similarity between the two documents\n",
    "    '''\n",
    "    d_i = np.array(TFIDF[:, i])\n",
    "    d_j = np.array(TFIDF[:, j])\n",
    "    \n",
    "    return (d_i @ d_j) / (np.linalg.norm(d_i) * np.linalg.norm(d_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docSimSearch(name):\n",
    "    '''\n",
    "    Finds the top 5 courses with the highest score for the term given\n",
    "    \n",
    "    Parameter:\n",
    "        name (string): the name of the term\n",
    "    '''\n",
    "    courseIdx = termToIdx[name]\n",
    "    courseTop5 = np.argsort(-TFIDF[courseIdx])[:5]\n",
    "    combs = combinations(courseTop5, 2)\n",
    "\n",
    "    simScores = []\n",
    "    for comb in combs:\n",
    "        i = comb[0]\n",
    "        j = comb[1]\n",
    "        simScores.append((sim(i, j), idxToDoc[i], idxToDoc[j]))\n",
    "\n",
    "    print(f'----- Top five courses for \"{name}\" (Course: Score) ----- \\n')\n",
    "    for i, idx in enumerate(courseTop5):\n",
    "        print(f'{i+1}. {idxToDoc[idx]}: {TFIDF[courseIdx, idx]:.4f}')\n",
    "        \n",
    "    print('\\n----- Similarities between courses (Course 1 and Course 2: Similarity score) -----\\n')\n",
    "    for simScore in simScores:\n",
    "        print(f'{simScore[1]} and {simScore[2]}: {simScore[0]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Top five courses for \"markov chain\" (Course: Score) ----- \n",
      "\n",
      "1. MATH-332: 0.1890\n",
      "2. COM-516: 0.1046\n",
      "3. MGT-484: 0.0984\n",
      "4. MATH-600: 0.0541\n",
      "5. COM-512: 0.0304\n",
      "\n",
      "----- Similarities between courses (Course 1 and Course 2: Similarity score) -----\n",
      "\n",
      "MATH-332 and COM-516: 0.3214\n",
      "MATH-332 and MGT-484: 0.2780\n",
      "MATH-332 and MATH-600: 0.1557\n",
      "MATH-332 and COM-512: 0.1229\n",
      "COM-516 and MGT-484: 0.2719\n",
      "COM-516 and MATH-600: 0.2126\n",
      "COM-516 and COM-512: 0.2028\n",
      "MGT-484 and MATH-600: 0.0900\n",
      "MGT-484 and COM-512: 0.1113\n",
      "MATH-600 and COM-512: 0.0288\n"
     ]
    }
   ],
   "source": [
    "docSimSearch(\"markov chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the top courses are all math-related courses or courses in which we use graph algorithms which makes sens for markov chains. The similarity between them is good, which also makes sense since they share topics like stochastic processes or graph algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Top five courses for \"facebook\" (Course: Score) ----- \n",
      "\n",
      "1. EE-727: 0.1173\n",
      "2. MICRO-453: 0.0000\n",
      "3. BIO-629: 0.0000\n",
      "4. CS-596: 0.0000\n",
      "5. AR-482: 0.0000\n",
      "\n",
      "----- Similarities between courses (Course 1 and Course 2: Similarity score) -----\n",
      "\n",
      "EE-727 and MICRO-453: 0.0056\n",
      "EE-727 and BIO-629: 0.0174\n",
      "EE-727 and CS-596: 0.0162\n",
      "EE-727 and AR-482: 0.0196\n",
      "MICRO-453 and BIO-629: 0.0292\n",
      "MICRO-453 and CS-596: 0.0136\n",
      "MICRO-453 and AR-482: 0.0377\n",
      "BIO-629 and CS-596: 0.0113\n",
      "BIO-629 and AR-482: 0.0091\n",
      "CS-596 and AR-482: 0.0251\n"
     ]
    }
   ],
   "source": [
    "docSimSearch(\"facebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observe that the only course mentioning 'facebook' is EE-727. Obviously then, the similarity score between courses is very low since we don't really know how the 4 remaining courses were chosen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
