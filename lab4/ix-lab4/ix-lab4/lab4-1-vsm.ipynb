{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *K*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Mathieu Sauser*\n",
    "* *Luca Mouchel*\n",
    "* *Jérémy Chaverot*\n",
    "* *Heikel Jebali*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jebali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = {}\n",
    "\n",
    "for i, course in enumerate(courses):\n",
    "    description = course['description']\n",
    "    description = [char.lower() for char in description]\n",
    "    description = ''.join(description)\n",
    "\n",
    "    # Step 4: Remove punctuation marks\n",
    "    description = re.sub(r'[^\\w\\s]', '', description)\n",
    "\n",
    "    # Step 5: Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(description)\n",
    "    \n",
    "    # Step 6: Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords]                        \n",
    "        \n",
    "    # Step 7: Stem or lemmatize words\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmedTokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    freqDist = FreqDist(stemmedTokens)\n",
    "    for token in stemmedTokens:\n",
    "        if token not in freqs.keys():\n",
    "            freqs[token] = freqDist[token]\n",
    "        else:\n",
    "            freqs[token] += freqDist[token]\n",
    "\n",
    "    # Step 9: Add n-grams to the vocabulary\n",
    "    nGramRange = (2, 3)  # Specify the range of n-grams to consider\n",
    "    \n",
    "    ngrams = []\n",
    "    for i in range(nGramRange[0], nGramRange[1] + 1):\n",
    "        ngrams.extend(list(nltk.ngrams(stemmedTokens, i)))\n",
    "            \n",
    "    vocabulary = stemmedTokens + [' '.join(ngram) for ngram in ngrams]\n",
    "    \n",
    "    course['description'] = vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student is the most used (stemmed) word, with 9887 apparitions\n"
     ]
    }
   ],
   "source": [
    "mostFreq = float('-inf')\n",
    "mostFreqToken = ''\n",
    "for token, freq in freqs.items():\n",
    "    if freq > mostFreq: \n",
    "        mostFreq = freq\n",
    "        mostFreqToken = token\n",
    "\n",
    "print(f'{mostFreqToken} is the most used (stemmed) word, with {mostFreq} apparitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn', 'student', 'model', 'method', 'system']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqWords = [word for word in freqs.keys() if freqs[word] > mostFreq * 0.6]\n",
    "infreqWords = [word for word in freqs.keys() if freqs[word] < 4]\n",
    "\n",
    "freqWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '20 midterm', '20 midterm 30', '30', '30 final', '30 final exam', '50', 'acquir', 'acquir lectur', 'acquir lectur handson', 'activ', 'activ lectur', 'activ lectur homework', 'ad', 'ad', 'ad auction', 'ad auction', 'ad auction learn', 'ad auction provid', 'advertis class', 'advertis class explor', 'algebra', 'algebra', 'algebra algorithm', 'algebra algorithm data', 'algebra markov', 'algebra markov chain', 'algorithm', 'algorithm', 'algorithm data', 'algorithm data structur', 'algorithm statist', 'algorithm statist graph', 'analysi', 'analysi user', 'analysi user data', 'analyt', 'analyt', 'analyt applic', 'analyt applic social', 'analyt collect', 'analyt collect model', 'apach spark', 'apach spark keyword', 'applic', 'applic', 'applic inspir', 'applic inspir current', 'applic social', 'applic social network', 'assess', 'assess method', 'assess method project', 'auction', 'auction', 'auction learn', 'auction learn prerequisit', 'auction provid', 'auction provid good', 'balanc', 'balanc foundat', 'balanc foundat basic', 'base', 'base', 'base hadoop', 'base hadoop apach', 'base number', 'base number largescal', 'basic', 'basic', 'basic', 'basic linear', 'basic linear algebra', 'basic materi', 'basic materi algorithm', 'basic model', 'basic model fundament', 'cathedra', 'cathedra homework', 'cathedra homework lab', 'chain', 'chain java', 'chain java learn', 'class', 'class', 'class', 'class explor', 'class explor number', 'class lab', 'class lab draw', 'class seek', 'class seek balanc', 'cloud', 'cloud servic', 'cloud servic specif', 'cluster', 'cluster', 'cluster commun', 'cluster commun', 'cluster commun detect', 'cluster commun detect', 'collect', 'collect model', 'collect model analysi', 'com300', 'com300 recommend', 'com300 recommend cours', 'combin', 'combin theoret', 'combin theoret materi', 'commun', 'commun', 'commun', 'commun com300', 'commun com300 recommend', 'commun detect', 'commun detect', 'commun detect searchretrievaltop', 'commun detect topic', 'comput', 'comput', 'comput ad', 'comput ad auction', 'comput onlin', 'comput onlin ad', 'concept', 'concept', 'concept lab', 'concept lab design', 'concept start', 'concept start graph', 'concret', 'concret realworld', 'concret realworld problem', 'content', 'content class', 'content class seek', 'cours', 'cours', 'cours basic', 'cours basic linear', 'cours stochast', 'cours stochast model', 'coverag', 'coverag main', 'coverag main data', 'curat class', 'curat class lab', 'current', 'current practic', 'current practic internet', 'data', 'data', 'data', 'data', 'data', 'data', 'data largescal', 'data largescal onlin', 'data mine', 'data mine', 'data mine', 'data mine analyt', 'data mine machin', 'data mine problem', 'data onlin', 'data onlin servicesdevelop', 'data structur', 'data structur import', 'dataset', 'dataset', 'dataset curat', 'dataset curat class', 'dataset real', 'dataset real world', 'decad', 'decad content', 'decad content class', 'dedic', 'dedic infrastructur', 'dedic infrastructur base', 'design', 'design explor', 'design explor practic', 'detect', 'detect', 'detect searchretrievaltop', 'detect searchretrievaltop model', 'detect topic', 'detect topic model', 'dimension', 'dimension reduct', 'dimension reduct stream', 'draw', 'draw knowledg', 'draw knowledg acquir', 'ecommerc', 'ecommerc', 'ecommerc search', 'ecommerc search advertis', 'ecommerc social', 'ecommerc social media', 'effect', 'effect modelsdatamin', 'effect modelsdatamin machin', 'effici', 'effici effect', 'effici effect modelsdatamin', 'end', 'end student', 'end student explor', 'exam', 'exam 50', 'expect', 'expect student', 'expect student activ', 'explor', 'explor', 'explor', 'explor', 'explor', 'explor basic', 'explor basic model', 'explor largescal', 'explor largescal dataset', 'explor number', 'explor number key', 'explor practic', 'explor practic question', 'explor realworld', 'explor realworld data', 'field', 'field realworld', 'field realworld applic', 'final', 'final exam', 'final exam 50', 'foundat', 'foundat basic', 'foundat basic materi', 'framework', 'framework model', 'framework model typic', 'function', 'function onlin', 'function onlin servic', 'fundament', 'fundament concept', 'fundament concept lab', 'good', 'good coverag', 'good coverag main', 'graph', 'graph', 'graph linear', 'graph linear algebra', 'graph theori', 'graph theori relat', 'hadoop', 'hadoop', 'hadoop apach', 'hadoop apach spark', 'hadoop recommend', 'hadoop recommend system', 'handson', 'handson selfcontain', 'handson selfcontain assess', 'homework', 'homework', 'homework explor', 'homework explor basic', 'homework lab', 'homework lab session', 'import', 'import concept', 'import concept start', 'inform', 'inform', 'inform network', 'inform network recommend', 'inform retriev', 'inform retriev stream', 'infrastructur', 'infrastructur base', 'infrastructur base hadoop', 'inspir', 'inspir current', 'inspir current practic', 'internet', 'internet', 'internet analyt', 'internet analyt collect', 'internet cloud', 'internet cloud servic', 'java', 'java learn', 'java learn outcom', 'key', 'key function', 'key function onlin', 'keyword', 'keyword data', 'keyword data mine', 'knowledg', 'knowledg acquir', 'knowledg acquir lectur', 'lab', 'lab', 'lab', 'lab design', 'lab design explor', 'lab draw', 'lab draw knowledg', 'lab session', 'lab session expect', 'laboratori', 'laboratori session', 'laboratori session explor', 'largescal', 'largescal', 'largescal', 'largescal dataset', 'largescal dataset real', 'largescal onlin', 'largescal onlin servic', 'largescal realworld', 'largescal realworld dataset', 'learn outcom', 'learn outcom end', 'learn prerequisit', 'learn prerequisit requir', 'learn social', 'learn social network', 'learn techniqu', 'learn techniqu concret', 'lectur', 'lectur', 'lectur handson', 'lectur handson selfcontain', 'lectur homework', 'lectur homework explor', 'linear', 'linear', 'linear algebra', 'linear algebra', 'linear algebra algorithm', 'linear algebra markov', 'machin', 'machin', 'machin learn', 'machin learn', 'machin learn social', 'machin learn techniqu', 'main', 'main data', 'main data mine', 'mapreduc', 'mapreduc hadoop', 'mapreduc hadoop recommend', 'markov', 'markov chain', 'markov chain java', 'materi', 'materi', 'materi algorithm', 'materi algorithm statist', 'materi weekli', 'materi weekli laboratori', 'media', 'media combin', 'media combin theoret', 'method cathedra', 'method cathedra homework', 'method project', 'method project 20', 'midterm', 'midterm 30', 'midterm 30 final', 'mine', 'mine', 'mine', 'mine analyt', 'mine analyt applic', 'mine machin', 'mine machin learn', 'mine problem', 'mine problem onlin', 'model analysi', 'model analysi user', 'model commun', 'model commun com300', 'model dimension', 'model dimension reduct', 'model fundament', 'model fundament concept', 'model inform', 'model inform retriev', 'model typic', 'model typic data', 'modelsdatamin machin', 'modelsdatamin machin learn', 'network', 'network', 'network', 'network', 'network ecommerc', 'network ecommerc', 'network ecommerc search', 'network ecommerc social', 'network mapreduc', 'network mapreduc hadoop', 'network recommend', 'network recommend system', 'number', 'number', 'number key', 'number key function', 'number largescal', 'number largescal realworld', 'onlin', 'onlin', 'onlin', 'onlin', 'onlin', 'onlin ad', 'onlin ad auction', 'onlin servic', 'onlin servic', 'onlin servic social', 'onlin servic ubiquit', 'onlin servicesanalyz', 'onlin servicesanalyz effici', 'onlin servicesdevelop', 'onlin servicesdevelop framework', 'outcom', 'outcom end', 'outcom end student', 'past', 'past decad', 'past decad content', 'practic', 'practic', 'practic internet', 'practic internet cloud', 'practic question', 'practic question base', 'prerequisit', 'prerequisit requir', 'prerequisit requir cours', 'problem', 'problem', 'problem onlin', 'problem onlin servicesanalyz', 'problem teach', 'problem teach method', 'project', 'project 20', 'project 20 midterm', 'provid', 'provid good', 'provid good coverag', 'question', 'question base', 'question base number', 'real', 'real world', 'real world work', 'realworld', 'realworld', 'realworld', 'realworld', 'realworld applic', 'realworld applic inspir', 'realworld data', 'realworld data onlin', 'realworld dataset', 'realworld dataset curat', 'realworld problem', 'realworld problem teach', 'recommend', 'recommend', 'recommend', 'recommend cours', 'recommend cours basic', 'recommend system', 'recommend system', 'recommend system cluster', 'recommend system cluster', 'reduct', 'reduct stream', 'reduct stream comput', 'relat', 'relat field', 'relat field realworld', 'requir', 'requir cours', 'requir cours stochast', 'retriev', 'retriev stream', 'retriev stream comput', 'search', 'search advertis', 'search advertis class', 'searchretrievaltop model', 'searchretrievaltop model dimension', 'seek', 'seek balanc', 'seek balanc foundat', 'selfcontain assess', 'selfcontain assess method', 'servic', 'servic', 'servic', 'servic social', 'servic social network', 'servic specif', 'servic specif social', 'servic ubiquit', 'servic ubiquit past', 'servicesanalyz effici', 'servicesanalyz effici effect', 'servicesdevelop framework', 'servicesdevelop framework model', 'session', 'session', 'session expect', 'session expect student', 'session explor', 'session explor largescal', 'social', 'social', 'social', 'social', 'social', 'social inform', 'social inform network', 'social media', 'social media combin', 'social network', 'social network', 'social network', 'social network ecommerc', 'social network ecommerc', 'social network mapreduc', 'spark keyword', 'spark keyword data', 'specif', 'specif social', 'specif social inform', 'start', 'start graph', 'start graph linear', 'statist', 'statist graph', 'statist graph theori', 'stochast', 'stochast model', 'stochast model commun', 'stream', 'stream', 'stream comput', 'stream comput', 'stream comput ad', 'stream comput onlin', 'structur', 'structur import', 'structur import concept', 'student activ', 'student activ lectur', 'student explor', 'student explor realworld', 'system cluster', 'system cluster', 'system cluster commun', 'system cluster commun', 'teach', 'teach method', 'teach method cathedra', 'techniqu', 'techniqu concret', 'techniqu concret realworld', 'theoret', 'theoret materi', 'theoret materi weekli', 'theori', 'theori relat', 'theori relat field', 'topic', 'topic model', 'topic model inform', 'typic', 'typic data', 'typic data mine', 'ubiquit', 'ubiquit past', 'ubiquit past decad', 'user', 'user data', 'user data largescal', 'weekli', 'weekli laboratori', 'weekli laboratori session', 'work', 'work dedic', 'work dedic infrastructur', 'world', 'world work', 'world work dedic']\n"
     ]
    }
   ],
   "source": [
    "for course in courses:\n",
    "    description = course['description']\n",
    "    course['description'] = [word for word in description if word not in freqWords and word not in infreqWords]\n",
    "    \n",
    "    if course['courseId'] == 'COM-308':\n",
    "        print(sorted(course['description']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 terms in the IX class description:\n",
      "network ecommerc\n",
      "ad auction\n",
      "recommend system cluster\n",
      "cluster commun detect\n",
      "system cluster commun\n",
      "system cluster\n",
      "social network ecommerc\n",
      "cluster commun\n",
      "mine\n",
      "social network\n",
      "data mine\n",
      "explor\n",
      "social\n",
      "realworld\n",
      "onlin\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Build vocabulary and document-term frequency matrix\n",
    "vocabulary = set()\n",
    "doc_term_freq = []\n",
    "ix_class_index = 0  # Replace with the actual index of the IX class\n",
    "\n",
    "for i, course in enumerate(courses):\n",
    "    description = course['description']\n",
    "    term_freq = FreqDist(description)\n",
    "    doc_term_freq.append(term_freq)\n",
    "    vocabulary.update(term_freq.keys())\n",
    "    \n",
    "    if course['courseId'] == 'COM-308': ix_class_index = i\n",
    "\n",
    "vocabulary = sorted(vocabulary)\n",
    "num_terms = len(vocabulary)\n",
    "num_docs = len(courses)\n",
    "\n",
    "# Step 2: Calculate inverse document frequency (IDF)\n",
    "idf = np.zeros(num_terms)\n",
    "for i, term in enumerate(vocabulary):\n",
    "    num_docs_with_term = sum(1 for freq in doc_term_freq if term in freq)\n",
    "    idf[i] = np.log(num_docs / (1 + num_docs_with_term))\n",
    "\n",
    "# Step 3: Compute TF-IDF scores\n",
    "tf_idf = np.zeros((num_terms, num_docs))\n",
    "for j, term_freq in enumerate(doc_term_freq):\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        tf = term_freq[term]\n",
    "        tf_idf[i, j] = tf * idf[i]\n",
    "\n",
    "# Step 4: Construct sparse term-document matrix\n",
    "X = csr_matrix(tf_idf)\n",
    "\n",
    "# Step 5: Print 15 terms with the highest TF-IDF scores in the IX class description\n",
    "ix_class_tfidf_scores = tf_idf[:, ix_class_index]\n",
    "top_terms_indices = np.argsort(ix_class_tfidf_scores)[-15:]\n",
    "top_terms = [vocabulary[i] for i in top_terms_indices]\n",
    "print(\"Top 15 terms in the IX class description:\")\n",
    "for term in top_terms:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
