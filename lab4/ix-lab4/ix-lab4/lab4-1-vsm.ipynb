{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *K*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Mathieu Sauser*\n",
    "* *Luca Mouchel*\n",
    "* *Jérémy Chaverot*\n",
    "* *Heikel Jebali*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jebali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jebali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = {}\n",
    "\n",
    "for i, course in enumerate(courses):\n",
    "    description = course['description']\n",
    "    description = [char.lower() for char in description]\n",
    "    description = ''.join(description)\n",
    "\n",
    "    # Step 4: Remove punctuation marks\n",
    "    description = re.sub(r'[^\\w\\s]', '', description)\n",
    "\n",
    "    # Step 5: Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(description)\n",
    "    \n",
    "    # Step 6: Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords]                        \n",
    "        \n",
    "    # Step 7: Stem or lemmatize words\n",
    "    #stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizedTokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    #stemmedTokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    #freqDist = FreqDist(stemmedTokens)\n",
    "    freqDist = FreqDist(lemmatizedTokens)\n",
    "    for token in lemmatizedTokens:\n",
    "        if token not in freqs.keys():\n",
    "            freqs[token] = freqDist[token]\n",
    "        else:\n",
    "            freqs[token] += freqDist[token]\n",
    "\n",
    "    # Step 9: Add n-grams to the vocabulary\n",
    "    nGramRange = (2, 3)  # Specify the range of n-grams to consider\n",
    "    \n",
    "    ngrams = []\n",
    "    for i in range(nGramRange[0], nGramRange[1] + 1):\n",
    "        ngrams.extend(list(nltk.ngrams(lemmatizedTokens, i)))\n",
    "            \n",
    "    vocabulary = lemmatizedTokens + [' '.join(ngram) for ngram in ngrams]\n",
    "    \n",
    "    course['description'] = vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**student** is the most used (lemmatized) word, with 9887 apparitions\n"
     ]
    }
   ],
   "source": [
    "mostFreq = float('-inf')\n",
    "mostFreqToken = ''\n",
    "for token, freq in freqs.items():\n",
    "    if freq > mostFreq: \n",
    "        mostFreq = freq\n",
    "        mostFreqToken = token\n",
    "\n",
    "print(f'**{mostFreqToken}** is the most used (lemmatized) word, with {mostFreq} apparitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words: ['student', 'method', 'system']\n"
     ]
    }
   ],
   "source": [
    "freqWords = [word for word in freqs.keys() if freqs[word] > mostFreq * 0.6]\n",
    "infreqWords = [word for word in freqs.keys() if freqs[word] < 4]\n",
    "\n",
    "print(f'Most frequent words: {freqWords}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '20 midterm', '20 midterm 30', '30', '30 final', '30 final exam', '50', 'acquired', 'acquired lecture', 'acquired lecture handson', 'activity', 'activity lecture', 'activity lecture homework', 'ad', 'ad', 'ad auction', 'ad auction', 'ad auction learning', 'ad auction provide', 'advertisement class', 'advertisement class explores', 'algebra', 'algebra', 'algebra algorithm', 'algebra algorithm data', 'algebra markov', 'algebra markov chain', 'algorithm', 'algorithm', 'algorithm data', 'algorithm data structure', 'algorithm statistic', 'algorithm statistic graph', 'analysis', 'analysis user', 'analysis user data', 'analytics', 'analytics', 'analytics application', 'analytics application social', 'analytics collection', 'analytics collection modeling', 'apache spark', 'apache spark keywords', 'application', 'application', 'application inspired', 'application inspired current', 'application social', 'application social networking', 'assessment', 'assessment method', 'assessment method project', 'auction', 'auction', 'auction learning', 'auction learning prerequisite', 'auction provide', 'auction provide good', 'balance', 'balance foundational', 'balance foundational basic', 'based', 'based', 'based hadoop', 'based hadoop apache', 'based number', 'based number largescale', 'basic', 'basic', 'basic', 'basic linear', 'basic linear algebra', 'basic material', 'basic material algorithm', 'basic model', 'basic model fundamental', 'cathedra', 'cathedra homework', 'cathedra homework lab', 'chain', 'chain java', 'chain java learning', 'class', 'class', 'class', 'class explores', 'class explores number', 'class lab', 'class lab draw', 'class seek', 'class seek balance', 'cloud', 'cloud service', 'cloud service specifically', 'clustering', 'clustering', 'clustering community', 'clustering community', 'clustering community detection', 'clustering community detection', 'collection', 'collection modeling', 'collection modeling analysis', 'com300', 'com300 recommended', 'com300 recommended course', 'combination', 'combination theoretical', 'combination theoretical material', 'communication', 'communication com300', 'communication com300 recommended', 'community', 'community', 'community detection', 'community detection', 'community detection searchretrievaltopic', 'community detection topic', 'computing', 'computing', 'computing ad', 'computing ad auction', 'computing online', 'computing online ad', 'concept', 'concept', 'concept lab', 'concept lab designed', 'concept start', 'concept start graph', 'concrete', 'concrete realworld', 'concrete realworld problem', 'content', 'content class', 'content class seek', 'course', 'course', 'course basic', 'course basic linear', 'course stochastic', 'course stochastic model', 'coverage', 'coverage main', 'coverage main data', 'curated class', 'curated class lab', 'current', 'current practice', 'current practice internet', 'data', 'data', 'data', 'data', 'data', 'data', 'data largescale', 'data largescale online', 'data mining', 'data mining', 'data mining', 'data mining analytics', 'data mining machine', 'data mining problem', 'data online', 'data online servicesdevelop', 'data structure', 'data structure important', 'datasets', 'datasets', 'datasets curated', 'datasets curated class', 'datasets real', 'datasets real world', 'decade', 'decade content', 'decade content class', 'dedicated', 'dedicated infrastructure', 'dedicated infrastructure based', 'designed', 'designed explore', 'designed explore practical', 'detection', 'detection', 'detection searchretrievaltopic', 'detection searchretrievaltopic model', 'detection topic', 'detection topic model', 'dimensionality', 'dimensionality reduction', 'dimensionality reduction stream', 'draw', 'draw knowledge', 'draw knowledge acquired', 'ecommerce', 'ecommerce', 'ecommerce search', 'ecommerce search advertisement', 'ecommerce social', 'ecommerce social medium', 'effectiveness', 'effectiveness modelsdatamining', 'effectiveness modelsdatamining machine', 'efficiency', 'efficiency effectiveness', 'efficiency effectiveness modelsdatamining', 'end', 'end student', 'end student explore', 'exam', 'exam 50', 'expected', 'expected student', 'expected student activity', 'explore', 'explore', 'explore', 'explore', 'explore basic', 'explore basic model', 'explore largescale', 'explore largescale datasets', 'explore practical', 'explore practical question', 'explore realworld', 'explore realworld data', 'explores', 'explores number', 'explores number key', 'field', 'field realworld', 'field realworld application', 'final', 'final exam', 'final exam 50', 'foundational basic', 'foundational basic material', 'framework', 'framework model', 'framework model typical', 'function', 'function online', 'function online service', 'fundamental', 'fundamental concept', 'fundamental concept lab', 'good', 'good coverage', 'good coverage main', 'graph', 'graph', 'graph linear', 'graph linear algebra', 'graph theory', 'graph theory related', 'hadoop', 'hadoop', 'hadoop apache', 'hadoop apache spark', 'hadoop recommender', 'hadoop recommender system', 'handson', 'handson selfcontained', 'handson selfcontained assessment', 'homework', 'homework', 'homework explore', 'homework explore basic', 'homework lab', 'homework lab session', 'important', 'important concept', 'important concept start', 'information', 'information', 'information network', 'information network recommender', 'information retrieval', 'information retrieval stream', 'infrastructure', 'infrastructure based', 'infrastructure based hadoop', 'inspired current', 'inspired current practice', 'internet', 'internet', 'internet analytics', 'internet analytics collection', 'internet cloud', 'internet cloud service', 'java', 'java learning', 'java learning outcome', 'key', 'key function', 'key function online', 'keywords', 'keywords data', 'keywords data mining', 'knowledge', 'knowledge acquired', 'knowledge acquired lecture', 'lab', 'lab', 'lab', 'lab designed', 'lab designed explore', 'lab draw', 'lab draw knowledge', 'lab session', 'lab session expected', 'laboratory', 'laboratory session', 'laboratory session explore', 'largescale', 'largescale', 'largescale', 'largescale datasets', 'largescale datasets real', 'largescale online', 'largescale online service', 'largescale realworld', 'largescale realworld datasets', 'learning', 'learning', 'learning', 'learning', 'learning outcome', 'learning outcome end', 'learning prerequisite', 'learning prerequisite required', 'learning social', 'learning social networking', 'learning technique', 'learning technique concrete', 'lecture', 'lecture', 'lecture handson', 'lecture handson selfcontained', 'lecture homework', 'lecture homework explore', 'linear', 'linear', 'linear algebra', 'linear algebra', 'linear algebra algorithm', 'linear algebra markov', 'machine', 'machine', 'machine learning', 'machine learning', 'machine learning social', 'machine learning technique', 'main', 'main data', 'main data mining', 'mapreduce', 'mapreduce hadoop', 'mapreduce hadoop recommender', 'markov', 'markov chain', 'markov chain java', 'material', 'material', 'material algorithm', 'material algorithm statistic', 'material weekly', 'material weekly laboratory', 'medium', 'medium combination', 'medium combination theoretical', 'method cathedra', 'method cathedra homework', 'method project', 'method project 20', 'midterm', 'midterm 30', 'midterm 30 final', 'mining', 'mining', 'mining', 'mining analytics', 'mining analytics application', 'mining machine', 'mining machine learning', 'mining problem', 'mining problem online', 'model', 'model', 'model', 'model', 'model', 'model communication', 'model communication com300', 'model dimensionality', 'model dimensionality reduction', 'model fundamental', 'model fundamental concept', 'model information', 'model information retrieval', 'model typical', 'model typical data', 'modeling', 'modeling analysis', 'modeling analysis user', 'modelsdatamining machine', 'modelsdatamining machine learning', 'network', 'network recommender', 'network recommender system', 'networking', 'networking', 'networking', 'networking ecommerce', 'networking ecommerce', 'networking ecommerce search', 'networking ecommerce social', 'networking mapreduce', 'networking mapreduce hadoop', 'number', 'number', 'number key', 'number key function', 'number largescale', 'number largescale realworld', 'online', 'online', 'online', 'online', 'online', 'online ad', 'online ad auction', 'online service', 'online service', 'online service social', 'online service ubiquitous', 'online servicesanalyze', 'online servicesanalyze efficiency', 'online servicesdevelop', 'online servicesdevelop framework', 'outcome', 'outcome end', 'outcome end student', 'past', 'past decade', 'past decade content', 'practical', 'practical question', 'practical question based', 'practice', 'practice internet', 'practice internet cloud', 'prerequisite', 'prerequisite required', 'prerequisite required course', 'problem', 'problem', 'problem online', 'problem online servicesanalyze', 'problem teaching', 'problem teaching method', 'project', 'project 20', 'project 20 midterm', 'provide', 'provide good', 'provide good coverage', 'question', 'question based', 'question based number', 'real', 'real world', 'real world work', 'realworld', 'realworld', 'realworld', 'realworld', 'realworld application', 'realworld application inspired', 'realworld data', 'realworld data online', 'realworld datasets', 'realworld datasets curated', 'realworld problem', 'realworld problem teaching', 'recommended', 'recommended course', 'recommended course basic', 'recommender', 'recommender', 'recommender system', 'recommender system', 'recommender system clustering', 'recommender system clustering', 'reduction', 'reduction stream', 'reduction stream computing', 'related', 'related field', 'related field realworld', 'required', 'required course', 'required course stochastic', 'retrieval', 'retrieval stream', 'retrieval stream computing', 'search', 'search advertisement', 'search advertisement class', 'searchretrievaltopic model', 'searchretrievaltopic model dimensionality', 'seek balance', 'seek balance foundational', 'selfcontained assessment', 'selfcontained assessment method', 'service', 'service', 'service', 'service social', 'service social networking', 'service specifically', 'service specifically social', 'service ubiquitous', 'service ubiquitous past', 'servicesanalyze efficiency', 'servicesanalyze efficiency effectiveness', 'servicesdevelop framework', 'servicesdevelop framework model', 'session', 'session', 'session expected', 'session expected student', 'session explore', 'session explore largescale', 'social', 'social', 'social', 'social', 'social', 'social information', 'social information network', 'social medium', 'social medium combination', 'social networking', 'social networking', 'social networking', 'social networking ecommerce', 'social networking ecommerce', 'social networking mapreduce', 'spark keywords', 'spark keywords data', 'specifically', 'specifically social', 'specifically social information', 'start', 'start graph', 'start graph linear', 'statistic', 'statistic graph', 'statistic graph theory', 'stochastic', 'stochastic model', 'stochastic model communication', 'stream', 'stream', 'stream computing', 'stream computing', 'stream computing ad', 'stream computing online', 'structure', 'structure important', 'structure important concept', 'student activity', 'student activity lecture', 'student explore', 'student explore realworld', 'system clustering', 'system clustering', 'system clustering community', 'system clustering community', 'teaching', 'teaching method', 'teaching method cathedra', 'technique', 'technique concrete', 'technique concrete realworld', 'theoretical', 'theoretical material', 'theoretical material weekly', 'theory', 'theory related', 'theory related field', 'topic', 'topic model', 'topic model information', 'typical', 'typical data', 'typical data mining', 'ubiquitous', 'ubiquitous past', 'ubiquitous past decade', 'user', 'user data', 'user data largescale', 'weekly', 'weekly laboratory', 'weekly laboratory session', 'work', 'work dedicated', 'work dedicated infrastructure', 'world', 'world work', 'world work dedicated']\n"
     ]
    }
   ],
   "source": [
    "for course in courses:\n",
    "    description = course['description']\n",
    "    course['description'] = [word for word in description if word not in freqWords and word not in infreqWords]\n",
    "    \n",
    "    if course['courseId'] == 'COM-308':\n",
    "        print(sorted(course['description']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "termToIdx = {}\n",
    "for i, term in enumerate(freqs.keys()):\n",
    "    termToIdx[term] = i\n",
    "    \n",
    "docToIdx = {}\n",
    "for i, course in enumerate(courses):\n",
    "    docToIdx[course['courseId']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(termToIdx.keys())\n",
    "N = len(docToIdx.keys())\n",
    "TD = np.zeros((M, N))\n",
    "\n",
    "for course in courses:\n",
    "    for term in course['description']:\n",
    "        splittedNgrams = term.split()\n",
    "        for word in splittedNgrams:\n",
    "            TD[termToIdx[word], docToIdx[course['courseId']]] += 1\n",
    "        \n",
    "termsPerDoc = np.sum(TD, axis=0)\n",
    "TF = TD / termsPerDoc\n",
    "IDF = np.log2(N / np.count_nonzero(TD, axis=1))\n",
    "TFIDF = np.transpose(np.transpose(TF) * IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./TFIDF.npy', TFIDF)\n",
    "np.save('./termToIdx.npy', termToIdx)\n",
    "np.save('./docToIdx.npy', docToIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Top 15 terms with highest TF-IDF scores in IX course ------\n",
      "\n",
      "------------------\n",
      "Term: Score\n",
      "------------------\n",
      "online: 0.1187\n",
      "realworld: 0.1185\n",
      "social: 0.1110\n",
      "explore: 0.1030\n",
      "mining: 0.0973\n",
      "networking: 0.0927\n",
      "hadoop: 0.0842\n",
      "largescale: 0.0829\n",
      "ecommerce: 0.0785\n",
      "recommender: 0.0785\n",
      "service: 0.0753\n",
      "auction: 0.0745\n",
      "datasets: 0.0714\n",
      "stream: 0.0649\n",
      "data: 0.0635\n"
     ]
    }
   ],
   "source": [
    "ixScores = np.argsort(-TFIDF[:, docToIdx['COM-308']])\n",
    "top15idx = ixScores[:15]\n",
    "\n",
    "print('------ Top 15 terms with highest TF-IDF scores in IX course ------\\n')\n",
    "print('------------------')\n",
    "print('Term: Score')\n",
    "print('------------------')\n",
    "for idx in top15idx:\n",
    "    for term, idx2 in termToIdx.items():\n",
    "        if idx == idx2:\n",
    "            print(f'{term}: {TFIDF[idx, docToIdx[\"COM-308\"]]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
