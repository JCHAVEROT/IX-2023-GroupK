{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *K*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Mathieu Sauser*\n",
    "* *Luca Mouchel*\n",
    "* *Jérémy Chaverot*\n",
    "* *Heikel Jebali*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jebali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jebali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = {}\n",
    "\n",
    "for i, course in enumerate(courses):\n",
    "    description = course['description']\n",
    "    description = [char.lower() for char in description]\n",
    "    description = ''.join(description)\n",
    "\n",
    "    # Step 4: Remove punctuation marks\n",
    "    description = re.sub(r'[^\\w\\s]', '', description)\n",
    "\n",
    "    # Step 5: Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(description)\n",
    "    \n",
    "    # Step 6: Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords]                        \n",
    "        \n",
    "    # Step 7: Stem or lemmatize words\n",
    "    #stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizedTokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    #stemmedTokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    #freqDist = FreqDist(stemmedTokens)\n",
    "    freqDist = FreqDist(lemmatizedTokens)\n",
    "    for token in lemmatizedTokens:\n",
    "        if token not in freqs.keys():\n",
    "            freqs[token] = freqDist[token]\n",
    "        else:\n",
    "            freqs[token] += freqDist[token]\n",
    "\n",
    "    # Step 9: Add n-grams to the vocabulary\n",
    "    nGramRange = (2, 3)  # Specify the range of n-grams to consider\n",
    "    \n",
    "    ngrams = []\n",
    "    for i in range(nGramRange[0], nGramRange[1] + 1):\n",
    "        ngrams.extend(list(nltk.ngrams(lemmatizedTokens, i)))\n",
    "        \n",
    "    ngrams = [' '.join(ngram) for ngram in ngrams]\n",
    "    for ngram in ngrams:\n",
    "        if ngram not in freqs.keys():\n",
    "            freqs[ngram] = 1\n",
    "        else:\n",
    "            freqs[ngram] += 1\n",
    "            \n",
    "    vocabulary = lemmatizedTokens + ngrams\n",
    "    \n",
    "    course['description'] = vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**student** is the most used (lemmatized) word, with 9887 apparitions\n"
     ]
    }
   ],
   "source": [
    "mostFreq = float('-inf')\n",
    "mostFreqToken = ''\n",
    "for token, freq in freqs.items():\n",
    "    if freq > mostFreq: \n",
    "        mostFreq = freq\n",
    "        mostFreqToken = token\n",
    "\n",
    "print(f'**{mostFreqToken}** is the most used (lemmatized) word, with {mostFreq} apparitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words: ['student', 'method', 'system']\n"
     ]
    }
   ],
   "source": [
    "freqWords = [word for word in freqs.keys() if freqs[word] > mostFreq * 0.6]\n",
    "infreqWords = [word for word in freqs.keys() if freqs[word] < 4]\n",
    "\n",
    "print(f'Most frequent words: {freqWords}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '20 midterm', '30', '30 final', '30 final exam', '50', 'acquired', 'activity', 'activity lecture', 'ad', 'ad', 'algebra', 'algebra', 'algorithm', 'algorithm', 'algorithm data', 'algorithm data structure', 'analysis', 'analytics', 'analytics', 'application', 'application', 'assessment', 'assessment method', 'assessment method project', 'auction', 'auction', 'balance', 'based', 'based', 'basic', 'basic', 'basic', 'basic linear', 'basic linear algebra', 'basic material', 'basic model', 'cathedra', 'chain', 'class', 'class', 'class', 'cloud', 'clustering', 'clustering', 'collection', 'com300', 'combination', 'communication', 'communication com300', 'community', 'community', 'computing', 'computing', 'concept', 'concept', 'concept start', 'concrete', 'content', 'content class', 'course', 'course', 'course basic', 'course stochastic', 'course stochastic model', 'coverage', 'current', 'data', 'data', 'data', 'data', 'data', 'data', 'data mining', 'data mining', 'data mining', 'data structure', 'datasets', 'datasets', 'decade', 'dedicated', 'designed', 'detection', 'detection', 'dimensionality', 'dimensionality reduction', 'draw', 'ecommerce', 'ecommerce', 'effectiveness', 'efficiency', 'end', 'end student', 'end student explore', 'exam', 'exam 50', 'expected', 'expected student', 'expected student activity', 'explore', 'explore', 'explore', 'explore', 'explores', 'field', 'final', 'final exam', 'final exam 50', 'framework', 'function', 'fundamental', 'fundamental concept', 'good', 'graph', 'graph', 'graph theory', 'hadoop', 'hadoop', 'handson', 'homework', 'homework', 'important', 'important concept', 'important concept start', 'information', 'information', 'information retrieval', 'infrastructure', 'internet', 'internet', 'java', 'key', 'keywords', 'keywords data', 'knowledge', 'knowledge acquired', 'lab', 'lab', 'lab', 'lab session', 'laboratory', 'laboratory session', 'largescale', 'largescale', 'largescale', 'learning', 'learning', 'learning', 'learning', 'learning outcome', 'learning outcome end', 'learning prerequisite', 'learning prerequisite required', 'learning technique', 'lecture', 'lecture', 'lecture homework', 'linear', 'linear', 'linear algebra', 'linear algebra', 'machine', 'machine', 'machine learning', 'machine learning', 'machine learning technique', 'main', 'mapreduce', 'markov', 'markov chain', 'material', 'material', 'medium', 'method cathedra', 'method project', 'midterm', 'mining', 'mining', 'mining', 'model', 'model', 'model', 'model', 'model', 'model communication', 'model communication com300', 'modeling', 'modeling analysis', 'network', 'networking', 'networking', 'networking', 'number', 'number', 'online', 'online', 'online', 'online', 'online', 'outcome', 'outcome end', 'outcome end student', 'past', 'practical', 'practical question', 'practice', 'prerequisite', 'prerequisite required', 'prerequisite required course', 'problem', 'problem', 'problem teaching', 'problem teaching method', 'project', 'provide', 'question', 'real', 'real world', 'realworld', 'realworld', 'realworld', 'realworld', 'realworld application', 'recommended', 'recommended course', 'recommended course basic', 'recommender', 'recommender', 'recommender system', 'recommender system', 'reduction', 'related', 'related field', 'required', 'required course', 'required course stochastic', 'retrieval', 'search', 'service', 'service', 'service', 'session', 'session', 'session expected', 'session expected student', 'social', 'social', 'social', 'social', 'social', 'social medium', 'specifically', 'start', 'statistic', 'stochastic', 'stochastic model', 'stochastic model communication', 'stream', 'stream', 'structure', 'student activity', 'student activity lecture', 'student explore', 'teaching', 'teaching method', 'teaching method cathedra', 'technique', 'theoretical', 'theory', 'topic', 'typical', 'ubiquitous', 'user', 'weekly', 'work', 'world']\n"
     ]
    }
   ],
   "source": [
    "for course in courses:\n",
    "    description = course['description']\n",
    "    course['description'] = [word for word in description if word not in freqWords and word not in infreqWords]\n",
    "    \n",
    "    if course['courseId'] == 'COM-308':\n",
    "        print(sorted(course['description']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "termToIdx = {}\n",
    "for i, term in enumerate(freqs.keys()):\n",
    "    termToIdx[term] = i\n",
    "    \n",
    "docToIdx = {}\n",
    "for i, course in enumerate(courses):\n",
    "    docToIdx[course['courseId']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3.7/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/anaconda3.7/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in multiply\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "M = len(termToIdx.keys())\n",
    "N = len(docToIdx.keys())\n",
    "TD = np.zeros((M, N))\n",
    "\n",
    "for course in courses:\n",
    "    for term in course['description']:\n",
    "        TD[termToIdx[term], docToIdx[course['courseId']]] += 1\n",
    "        \n",
    "termsPerDoc = np.sum(TD, axis=0)\n",
    "TF = TD / termsPerDoc\n",
    "IDF = np.log2(N / np.count_nonzero(TD, axis=1))\n",
    "TFIDF = np.transpose(np.transpose(TF) * IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./TFIDF.npy', TFIDF)\n",
    "np.save('./termToIdx.npy', termToIdx)\n",
    "np.save('./docToIdx.npy', docToIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Top 15 terms with highest TF-IDF scores in IX course ------\n",
      "\n",
      "------------------\n",
      "Term: Score\n",
      "------------------\n",
      "online: 0.0893\n",
      "realworld: 0.0892\n",
      "social: 0.0835\n",
      "data mining: 0.0806\n",
      "explore: 0.0775\n",
      "mining: 0.0732\n",
      "networking: 0.0697\n",
      "hadoop: 0.0633\n",
      "largescale: 0.0624\n",
      "recommender system: 0.0591\n",
      "ecommerce: 0.0591\n",
      "recommender: 0.0591\n",
      "service: 0.0567\n",
      "auction: 0.0561\n",
      "datasets: 0.0537\n"
     ]
    }
   ],
   "source": [
    "ixScores = np.argsort(-TFIDF[:, docToIdx['COM-308']])\n",
    "top15idx = ixScores[:15]\n",
    "\n",
    "print('------ Top 15 terms with highest TF-IDF scores in IX course ------\\n')\n",
    "print('------------------')\n",
    "print('Term: Score')\n",
    "print('------------------')\n",
    "for idx in top15idx:\n",
    "    for term, idx2 in termToIdx.items():\n",
    "        if idx == idx2:\n",
    "            print(f'{term}: {TFIDF[idx, docToIdx[\"COM-308\"]]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
