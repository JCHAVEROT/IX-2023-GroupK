{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 3: Latent Dirichlet allocation\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *K*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Mathieu Sauser*\n",
    "* *Luca Mouchel*\n",
    "* *Heikel Jebali*\n",
    "* *Jérémy Chaverot*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 3 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some imports to begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from utils import load_json, load_pkl\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.8: Topics extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `preprocessed_courses.txt': File exists\n",
      "Found 4 items\n",
      "drwx------   - jchavero hadoop          0 2023-06-06 03:00 .Trash\n",
      "drwxr-xr-x   - jchavero hadoop          0 2023-06-06 19:43 .sparkStaging\n",
      "-rw-r--r--   3 jchavero hadoop    2147813 2023-03-01 09:39 election-day-tweets.txt\n",
      "-rw-r--r--   3 jchavero hadoop    2861332 2023-06-06 02:35 preprocessed_courses.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put data/preprocessed_courses.txt\n",
    "!hdfs dfs -ls\n",
    "\n",
    "# Load the pre-processed courses data\n",
    "courses_RDD = sc.textFile(\"preprocessed_courses.txt\").flatMap(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the identifier for all courses\n",
    "coursesIds_RDD = courses_RDD.map(lambda c: c['courseId']).distinct()\n",
    "\n",
    "# The number of distinct courses\n",
    "N = coursesIds_RDD.count() \n",
    "\n",
    "# Map each course to a unique index\n",
    "docToIdx = dict(zip(coursesIds_RDD.collect(), range(N)))\n",
    "docIdx = list(docToIdx.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all distinct terms from all documents\n",
    "all_words_RDD = courses_RDD.flatMap(lambda c: c['description']).distinct()\n",
    "\n",
    "# The number of distinct terms\n",
    "M = all_words_RDD.count() \n",
    "\n",
    "# Map each term to a unique index\n",
    "termToIdx = dict(zip(all_words_RDD.collect(), range(M)))\n",
    "vectorized_termToIdx = np.vectorize(lambda x: termToIdx[x])\n",
    "idxToTerm = {v: k for k, v in termToIdx.items()}\n",
    "\n",
    "# Compute a reduced version of the courses RDD with only indexes\n",
    "red_courses_RDD = courses_RDD.map(lambda c: (docToIdx[c[\"courseId\"]], vectorized_termToIdx(c[\"description\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  1: \u001b[1mPhysics\u001b[0m \n",
      "['method', 'equation', 'model', 'learning', 'numerical', 'exercise', 'basic']\n",
      "\n",
      "Topic  2: \u001b[1mElectromagnetism\u001b[0m \n",
      "['material', 'method', 'learning', 'property', 'magnetic', 'concept', 'energy']\n",
      "\n",
      "Topic  3: \u001b[1mMathematics\u001b[0m \n",
      "['data', 'learning', 'method', 'system', 'problem', 'analysis', 'algorithm']\n",
      "\n",
      "Topic  4: \u001b[1mOptical & Materials\u001b[0m \n",
      "['system', 'design', 'energy', 'material', 'learning', 'optical', 'method']\n",
      "\n",
      "Topic  5: \u001b[1mProject Planning\u001b[0m \n",
      "['project', 'report', 'research', 'skill', 'scientific', 'data', 'laboratory']\n",
      "\n",
      "Topic  6: \u001b[1mMethodology\u001b[0m \n",
      "['method', 'analysis', 'learning', 'content', 'note', 'exam', 'theory']\n",
      "\n",
      "Topic  7: \u001b[1mSignal Processing\u001b[0m \n",
      "['model', 'processing', 'learning', 'method', 'system', 'analysis', 'basic']\n",
      "\n",
      "Topic  8: \u001b[1mCourse content\u001b[0m \n",
      "['management', 'work', 'case', 'method', 'design', 'learning', 'presentation']\n",
      "\n",
      "Topic  9: \u001b[1mSystems Design\u001b[0m \n",
      "['system', 'design', 'circuit', 'technology', 'content', 'method', '1']\n",
      "\n",
      "Topic 10: \u001b[1mChemistry & Applications\u001b[0m \n",
      "['chemistry', 'molecular', 'system', 'application', 'method', 'process', 'biology']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Helper function to create the sparse vector of size M the number of terms for each doc\n",
    "def create_sparse_vector_from_document(doc):\n",
    "    vector = {}\n",
    "    for termIdx in doc[1]:\n",
    "        vector[termIdx] = vector.get(termIdx, 0) + 1\n",
    "    return (doc[0], Vectors.sparse(M, vector))\n",
    "\n",
    "# Build the term-document matrix\n",
    "term_doc_matrix = red_courses_RDD.map(lambda x: create_sparse_vector_from_document(x)).map(list)\n",
    "\n",
    "# Train the LDA model\n",
    "lda = LDA.train(term_doc_matrix, k=10, seed=0)\n",
    "\n",
    "# Function used to retrieve the words from their respective index\n",
    "def print_topics(model, idxToTerm=idxToTerm, labels=None, words_per_topic=7):\n",
    "    topics = model.describeTopics(words_per_topic)\n",
    "    for i, topic in enumerate(topics):\n",
    "        wordsIdx = topic[0]\n",
    "        words = []\n",
    "        for idx in wordsIdx:\n",
    "            word = idxToTerm[idx]\n",
    "            words.append(word)\n",
    "        if labels != None:\n",
    "            print(f'Topic {i + 1 :>2}: \\033[1m{labels[i]}\\033[0m \\n{words}\\n')\n",
    "        else: print(f'Topic {i + 1 :>2}: {words}')\n",
    "            \n",
    "\n",
    "# Infered labels for each topics, written by hand after a first print_topics() call\n",
    "labels = [\n",
    "    \"Physics\",\n",
    "    \"Electromagnetism\",\n",
    "    \"Mathematics\",\n",
    "    \"Optical & Materials\",\n",
    "    \"Project Planning\",\n",
    "    \"Methodology\",\n",
    "    \"Signal Processing\",\n",
    "    \"Course content\",\n",
    "    \"Systems Design\",\n",
    "    \"Chemistry & Applications\"\n",
    "]\n",
    "\n",
    "print_topics(lda, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing these 10 topics with the results of the previous lab on Latent Semantic Indexing (LSI), we can observe that there are only a few similarities. Specifically, the topics \"Electromagnetism\", \"Project Planning\" and \"Mathematics\" align with the LSI topics \"Electromagnetism\", \"Projects\" and \"Algebra\" respectively. \n",
    "\n",
    "It is worth mentioning that when utilizing either LSI or LDA, there is always some degree of inherent randomness involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.9 Dirichlet hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call the PySpark LDA function, $\\alpha$ corresponds to the $docConcentration$ parameter, and $\\beta$ corresponds to the $topicConcentration$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_VALUES = np.array([1.01, 2, 5, 10, 20, 100], dtype=np.float64)\n",
    "BETA_VALUES = np.array([1.01, 2, 3, 6, 10, 20], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, we fix $k=10$ and $\\beta=1.01$, and vary $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training of the RDD on the LDA model was completed while adjusting the alpha parameter.\n"
     ]
    }
   ],
   "source": [
    "def lda_vary_alpha(data, values=ALPHA_VALUES, beta=1.01, k=10, seed=0):\n",
    "    models = []\n",
    "    for alpha in values:\n",
    "        models.append(\n",
    "            LDA.train(\n",
    "                data, k=k, docConcentration=float(alpha), topicConcentration=float(beta), seed=seed\n",
    "            )\n",
    "        )\n",
    "    print(\"The training of the RDD on the LDA model was completed while adjusting the alpha parameter.\")\n",
    "    return models\n",
    "\n",
    "lda_alpha = lda_vary_alpha(term_doc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1malpha=1.01 & beta=1.01 : \u001b[0m\n",
      "Topic  1: ['method', 'learning', 'model', 'numerical', 'exercise', 'equation', 'course']\n",
      "Topic  2: ['method', 'material', 'learning', 'structure', 'property', 'project', 'content']\n",
      "Topic  3: ['learning', 'data', 'method', 'model', 'analysis', 'system', 'algorithm']\n",
      "Topic  4: ['system', 'energy', 'method', 'design', 'optical', 'learning', 'concept']\n",
      "Topic  5: ['report', 'project', 'research', 'learning', 'data', 'skill', 'laboratory']\n",
      "Topic  6: ['method', 'learning', 'analysis', 'content', 'signal', 'paper', 'exam']\n",
      "Topic  7: ['method', 'learning', 'processing', 'project', 'system', 'model', 'lecture']\n",
      "Topic  8: ['design', 'method', 'work', 'management', 'learning', 'case', 'project']\n",
      "Topic  9: ['design', 'circuit', 'system', 'device', 'content', 'method', 'learning']\n",
      "Topic 10: ['chemistry', 'system', 'project', 'method', 'content', 'engineering', 'learning']\n",
      "\n",
      "\n",
      "\u001b[1malpha=2.0 & beta=1.01 : \u001b[0m\n",
      "Topic  1: ['method', 'learning', 'model', 'numerical', 'equation', 'exercise', 'basic']\n",
      "Topic  2: ['material', 'method', 'learning', 'property', 'structure', 'concept', 'magnetic']\n",
      "Topic  3: ['data', 'learning', 'method', 'model', 'analysis', 'algorithm', 'problem']\n",
      "Topic  4: ['system', 'energy', 'design', 'optical', 'learning', 'method', 'laser']\n",
      "Topic  5: ['report', 'project', 'research', 'learning', 'data', 'skill', 'scientific']\n",
      "Topic  6: ['method', 'learning', 'analysis', 'content', 'signal', 'paper', 'exam']\n",
      "Topic  7: ['processing', 'method', 'learning', 'model', 'system', 'project', 'lecture']\n",
      "Topic  8: ['design', 'method', 'work', 'management', 'case', 'project', 'learning']\n",
      "Topic  9: ['design', 'circuit', 'system', 'device', 'content', 'technology', 'learning']\n",
      "Topic 10: ['chemistry', 'system', 'project', 'method', 'content', 'engineering', 'process']\n",
      "\n",
      "\n",
      "\u001b[1malpha=5.0 & beta=1.01 : \u001b[0m\n",
      "Topic  1: ['method', 'learning', 'equation', 'model', 'numerical', 'exercise', 'basic']\n",
      "Topic  2: ['material', 'method', 'energy', 'learning', 'concept', 'property', 'heat']\n",
      "Topic  3: ['data', 'learning', 'analysis', 'method', 'problem', 'model', 'algorithm']\n",
      "Topic  4: ['system', 'design', 'optical', 'energy', 'material', 'learning', 'method']\n",
      "Topic  5: ['project', 'research', 'report', 'learning', 'skill', 'scientific', 'data']\n",
      "Topic  6: ['method', 'analysis', 'learning', 'content', 'exam', 'note', 'theory']\n",
      "Topic  7: ['model', 'processing', 'method', 'learning', 'system', 'lecture', 'basic']\n",
      "Topic  8: ['design', 'method', 'work', 'case', 'management', 'project', 'learning']\n",
      "Topic  9: ['system', 'design', 'circuit', 'content', 'technology', '1', 'learning']\n",
      "Topic 10: ['chemistry', 'system', 'project', 'molecular', 'application', 'content', 'method']\n",
      "\n",
      "\n",
      "\u001b[1malpha=10.0 & beta=1.01 : \u001b[0m\n",
      "Topic  1: ['method', 'equation', 'numerical', 'learning', 'model', 'exercise', 'basic']\n",
      "Topic  2: ['method', 'energy', 'material', 'learning', 'concept', 'heat', 'magnetic']\n",
      "Topic  3: ['data', 'report', 'learning', 'problem', 'analysis', 'method', 'written']\n",
      "Topic  4: ['system', 'design', 'material', 'optical', 'learning', 'structure', 'method']\n",
      "Topic  5: ['project', 'research', 'learning', 'skill', 'scientific', 'work', 'group']\n",
      "Topic  6: ['method', 'analysis', 'learning', 'model', 'theory', 'exam', 'basic']\n",
      "Topic  7: ['model', 'processing', 'method', 'learning', 'lecture', 'system', 'analysis']\n",
      "Topic  8: ['design', 'method', 'case', 'work', 'management', 'learning', 'assessment']\n",
      "Topic  9: ['system', 'circuit', 'design', 'learning', 'content', 'method', 'engineering']\n",
      "Topic 10: ['chemistry', 'system', 'method', 'application', 'molecular', 'content', 'learning']\n",
      "\n",
      "\n",
      "\u001b[1malpha=20.0 & beta=1.01 : \u001b[0m\n",
      "Topic  1: ['method', 'learning', 'model', 'numerical', 'equation', 'system', 'basic']\n",
      "Topic  2: ['method', 'learning', 'energy', 'concept', 'magnetic', 'heat', 'system']\n",
      "Topic  3: ['data', 'report', 'project', 'problem', 'written', 'research', 'laboratory']\n",
      "Topic  4: ['design', 'material', 'system', 'method', 'learning', 'structure', 'device']\n",
      "Topic  5: ['project', 'learning', 'skill', 'research', 'work', 'scientific', 'method']\n",
      "Topic  6: ['method', 'learning', 'model', 'optic', 'basic', 'analysis', 'theory']\n",
      "Topic  7: ['model', 'method', 'learning', 'processing', 'system', 'lecture', 'content']\n",
      "Topic  8: ['method', 'learning', 'design', 'content', 'assessment', 'work', 'case']\n",
      "Topic  9: ['system', 'method', 'learning', 'content', 'engineering', 'lecture', 'design']\n",
      "Topic 10: ['method', 'system', 'learning', 'content', 'energy', 'application', 'process']\n",
      "\n",
      "\n",
      "\u001b[1malpha=100.0 & beta=1.01 : \u001b[0m\n",
      "Topic  1: ['method', 'learning', 'system', 'content', 'model', 'design', 'course']\n",
      "Topic  2: ['method', 'learning', 'system', 'content', 'course', 'design', 'model']\n",
      "Topic  3: ['report', 'method', 'learning', 'project', 'data', 'system', 'content']\n",
      "Topic  4: ['method', 'learning', 'system', 'content', 'design', 'course', 'model']\n",
      "Topic  5: ['method', 'learning', 'system', 'content', 'project', 'design', 'course']\n",
      "Topic  6: ['method', 'learning', 'system', 'content', 'model', 'analysis', 'course']\n",
      "Topic  7: ['method', 'learning', 'system', 'content', 'model', 'design', 'course']\n",
      "Topic  8: ['method', 'learning', 'system', 'content', 'design', 'course', 'model']\n",
      "Topic  9: ['method', 'learning', 'system', 'content', 'model', 'analysis', 'course']\n",
      "Topic 10: ['method', 'learning', 'system', 'content', 'model', 'design', 'course']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, alpha in enumerate(ALPHA_VALUES):\n",
    "    print(f'\\033[1malpha={alpha} & beta=1.01 : \\033[0m')\n",
    "    print_topics(lda_alpha[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "When we increase the value of $\\alpha$ in the LDA model, the topics will become more similar to each other. Conversely, decreasing the parameter will lead to more distinct and diverse topics.\n",
    "This is because as seen in class and in the spark documentation, $\\alpha$ controls the prior distribution of documents over each topic, and larger values encourage smoother inferred distributions, making them more uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then, we fix $k=10$ and $\\alpha=6$, and vary $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training of the RDD on the LDA model was completed while adjusting the beta parameter.\n"
     ]
    }
   ],
   "source": [
    "def lda_vary_beta(data, alpha=6, values=BETA_VALUES, k=10, seed=0):\n",
    "    models = []\n",
    "    for beta in values:\n",
    "        models.append(\n",
    "            LDA.train(\n",
    "                data, k=k, docConcentration=float(alpha), topicConcentration=float(beta), seed=seed\n",
    "            )\n",
    "        )\n",
    "    print(\"The training of the RDD on the LDA model was completed while adjusting the beta parameter.\")\n",
    "    return models\n",
    "\n",
    "lda_beta = lda_vary_beta(term_doc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1malpha=6 & beta=1.01 :\u001b[0m\n",
      "Topic  1: ['method', 'equation', 'learning', 'numerical', 'model', 'exercise', 'basic']\n",
      "Topic  2: ['method', 'material', 'energy', 'learning', 'concept', 'heat', 'property']\n",
      "Topic  3: ['data', 'learning', 'analysis', 'problem', 'method', 'report', 'algorithm']\n",
      "Topic  4: ['system', 'design', 'optical', 'material', 'energy', 'learning', 'method']\n",
      "Topic  5: ['project', 'research', 'learning', 'report', 'skill', 'scientific', 'work']\n",
      "Topic  6: ['method', 'analysis', 'learning', 'content', 'exam', 'theory', 'note']\n",
      "Topic  7: ['model', 'processing', 'method', 'learning', 'system', 'lecture', 'basic']\n",
      "Topic  8: ['design', 'method', 'work', 'case', 'management', 'learning', 'assessment']\n",
      "Topic  9: ['system', 'design', 'circuit', 'content', 'technology', 'learning', '1']\n",
      "Topic 10: ['chemistry', 'system', 'molecular', 'application', 'method', 'content', 'organic']\n",
      "\n",
      "\n",
      "\u001b[1malpha=6 & beta=2.0 :\u001b[0m\n",
      "Topic  1: ['method', 'learning', 'model', 'numerical', 'system', 'equation', 'content']\n",
      "Topic  2: ['method', 'learning', 'material', 'content', 'course', 'basic', 'concept']\n",
      "Topic  3: ['method', 'learning', 'system', 'energy', 'data', 'problem', 'analysis']\n",
      "Topic  4: ['design', 'system', 'learning', 'method', 'material', 'content', 'energy']\n",
      "Topic  5: ['project', 'report', 'research', 'learning', 'data', 'skill', 'scientific']\n",
      "Topic  6: ['method', 'learning', 'analysis', 'content', 'system', 'model', 'course']\n",
      "Topic  7: ['method', 'model', 'processing', 'learning', 'system', 'signal', 'basic']\n",
      "Topic  8: ['method', 'learning', 'risk', 'management', 'content', 'case', 'assessment']\n",
      "Topic  9: ['method', 'learning', 'system', 'content', 'course', 'basic', 'lecture']\n",
      "Topic 10: ['method', 'cell', 'learning', 'system', 'content', 'application', 'process']\n",
      "\n",
      "\n",
      "\u001b[1malpha=6 & beta=3.0 :\u001b[0m\n",
      "Topic  1: ['method', 'learning', 'system', 'model', 'content', 'design', 'lecture']\n",
      "Topic  2: ['method', 'learning', 'content', 'course', 'system', 'material', 'basic']\n",
      "Topic  3: ['method', 'learning', 'system', 'energy', 'model', 'content', 'analysis']\n",
      "Topic  4: ['method', 'learning', 'system', 'design', 'content', 'course', 'concept']\n",
      "Topic  5: ['learning', 'method', 'project', 'content', 'system', 'design', 'report']\n",
      "Topic  6: ['method', 'learning', 'system', 'analysis', 'content', 'model', 'course']\n",
      "Topic  7: ['method', 'learning', 'model', 'system', 'content', 'course', 'basic']\n",
      "Topic  8: ['method', 'learning', 'content', 'project', 'design', 'assessment', 'course']\n",
      "Topic  9: ['method', 'learning', 'system', 'content', 'model', 'course', 'basic']\n",
      "Topic 10: ['method', 'learning', 'system', 'content', 'course', 'cell', 'basic']\n",
      "\n",
      "\n",
      "\u001b[1malpha=6 & beta=6.0 :\u001b[0m\n",
      "Topic  1: ['method', 'learning', 'system', 'content', 'model', 'design', 'course']\n",
      "Topic  2: ['method', 'learning', 'content', 'system', 'course', 'model', 'design']\n",
      "Topic  3: ['method', 'learning', 'system', 'content', 'model', 'design', 'course']\n",
      "Topic  4: ['method', 'learning', 'system', 'design', 'content', 'course', 'analysis']\n",
      "Topic  5: ['method', 'learning', 'system', 'content', 'design', 'project', 'course']\n",
      "Topic  6: ['method', 'learning', 'system', 'content', 'analysis', 'model', 'course']\n",
      "Topic  7: ['method', 'learning', 'content', 'system', 'model', 'design', 'course']\n",
      "Topic  8: ['method', 'learning', 'content', 'design', 'course', 'system', 'model']\n",
      "Topic  9: ['method', 'learning', 'content', 'system', 'model', 'course', 'analysis']\n",
      "Topic 10: ['method', 'learning', 'system', 'content', 'design', 'course', 'model']\n",
      "\n",
      "\n",
      "\u001b[1malpha=6 & beta=10.0 :\u001b[0m\n",
      "Topic  1: ['method', 'learning', 'system', 'content', 'model', 'design', 'course']\n",
      "Topic  2: ['method', 'learning', 'system', 'content', 'course', 'design', 'model']\n",
      "Topic  3: ['method', 'learning', 'system', 'content', 'model', 'design', 'course']\n",
      "Topic  4: ['method', 'learning', 'system', 'content', 'design', 'course', 'model']\n",
      "Topic  5: ['method', 'learning', 'system', 'content', 'design', 'course', 'model']\n",
      "Topic  6: ['method', 'learning', 'system', 'content', 'model', 'course', 'design']\n",
      "Topic  7: ['method', 'learning', 'content', 'system', 'model', 'design', 'course']\n",
      "Topic  8: ['method', 'learning', 'content', 'system', 'design', 'model', 'course']\n",
      "Topic  9: ['method', 'learning', 'system', 'content', 'model', 'course', 'design']\n",
      "Topic 10: ['method', 'learning', 'system', 'content', 'design', 'model', 'course']\n",
      "\n",
      "\n",
      "\u001b[1malpha=6 & beta=20.0 :\u001b[0m\n",
      "Topic  1: ['method', 'learning', 'system', 'content', 'design', 'model', 'course']\n",
      "Topic  2: ['method', 'learning', 'system', 'content', 'design', 'model', 'course']\n",
      "Topic  3: ['method', 'learning', 'system', 'content', 'design', 'model', 'course']\n",
      "Topic  4: ['method', 'learning', 'system', 'content', 'design', 'course', 'model']\n",
      "Topic  5: ['method', 'learning', 'system', 'content', 'design', 'model', 'course']\n",
      "Topic  6: ['method', 'learning', 'system', 'content', 'model', 'design', 'course']\n",
      "Topic  7: ['method', 'learning', 'system', 'content', 'model', 'design', 'course']\n",
      "Topic  8: ['method', 'learning', 'system', 'content', 'model', 'design', 'course']\n",
      "Topic  9: ['method', 'learning', 'system', 'content', 'model', 'design', 'course']\n",
      "Topic 10: ['method', 'learning', 'system', 'content', 'design', 'model', 'course']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, beta in enumerate(BETA_VALUES):\n",
    "    print(f'\\033[1malpha=6 & beta={beta} :\\033[0m')\n",
    "    print_topics(lda_beta[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "In the same manner, when we increase the value of $\\beta$ in the LDA model, the topics will become more similar to each other. Conversely, decreasing the parameter will lead to more distinct and diverse topics.\n",
    "This is because as seen in class and in the spark documentation, $\\beta$ controls the prior distribution of topics over each term, and larger values encourage smoother inferred distributions, making them more uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.10: EPFL’s taught subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the outcomes obtained in the previous exercise, we can infer that the most interpretable results for representing EPFL's taught subjects require the following combination of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "alpha = 1.2\n",
    "beta = 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dirichlets hyperparameter choice explanation\n",
    "\n",
    "1. $k=15$\n",
    "\n",
    "At EPFL, we have a total of 13 distinct sections, each focusing on different areas of study and research. In addition to these sections, we can also take into account two more categories to cover external aspects. This comprehensive approach ensures that we consider the full spectrum of activities and departments within and beyond the academic environment of our school.\n",
    "\n",
    "2. $\\alpha = 1.2$ and $\\beta = 1.2$\n",
    "\n",
    "We want the topics to be as different as possible from each others, which means we have to give to $\\alpha$ and $\\beta$ the lowest value possible. But as the EPFL is an engineering school, some overlap is likely to happen between the topics discussed in the courses, and same goes for the terms used in the topics. That's why we put both $\\alpha=\\beta=1.2$ to have some tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  1: \u001b[1mComputer Science\u001b[0m \n",
      "['design', 'learning', 'system', 'project', 'method', 'programming', 'data', 'analysis', 'course', 'concept']\n",
      "\n",
      "Topic  2: \u001b[1mMathematics?\u001b[0m \n",
      "['energy', 'process', 'system', 'flow', 'heat', 'method', 'learning', 'transfer', 'concept', 'equation']\n",
      "\n",
      "Topic  3: \u001b[1mBiology/Life Sciences\u001b[0m \n",
      "['cell', 'biology', 'note', 'structure', 'system', 'material', 'learning', 'protein', 'content', 'molecular']\n",
      "\n",
      "Topic  4: \u001b[1mDigital Humanities\u001b[0m \n",
      "['research', 'semester', 'policy', 'learning', 'content', 'innovation', 'social', 'technology', 'industry', 'method']\n",
      "\n",
      "Topic  5: \u001b[1mScientific Research\u001b[0m \n",
      "['project', 'report', 'skill', 'week', 'data', 'scientific', 'written', 'problem', 'specific', 'research']\n",
      "\n",
      "Topic  6: \u001b[1mQuantum Sciences\u001b[0m \n",
      "['modeling', 'method', 'learning', 'model', 'quantum', 'exercise', 'system', 'design', 'theory', 'introduction']\n",
      "\n",
      "Topic  7: \u001b[1mEvaluation related stuff\u001b[0m \n",
      "['project', 'plan', 'learning', 'method', 'presentation', 'skill', 'work', 'group', 'oral', 'evaluate']\n",
      "\n",
      "Topic  8: \u001b[1mNuclear Physic\u001b[0m \n",
      "['reactor', 'laboratory', 'report', 'method', 'nuclear', 'analysis', 'waste', 'diffraction', 'line', 'health']\n",
      "\n",
      "Topic  9: \u001b[1mElectrical & Electronic Engineering\u001b[0m \n",
      "['method', 'control', 'model', 'learning', 'system', 'design', 'class', 'exercise', 'case', 'exam']\n",
      "\n",
      "Topic 10: \u001b[1mElectromagnetism\u001b[0m \n",
      "['optical', 'property', 'method', 'laser', 'application', 'material', 'physic', 'learning', 'principle', 'electron']\n",
      "\n",
      "Topic 11: \u001b[1mChemistry\u001b[0m \n",
      "['method', 'chemical', 'chemistry', 'exercise', 'lecture', 'molecular', 'activity', 'learning', 'exam', 'session']\n",
      "\n",
      "Topic 12: \u001b[1mData Science\u001b[0m \n",
      "['model', 'method', 'linear', 'algorithm', 'learning', 'system', 'signal', 'processing', 'basic', 'analysis']\n",
      "\n",
      "Topic 13: \u001b[1mMaterial Sciences\u001b[0m \n",
      "['material', 'circuit', 'method', 'analysis', 'design', 'content', 'structure', 'device', 'course', 'mechanical']\n",
      "\n",
      "Topic 14: \u001b[1mFinance Engineering\u001b[0m \n",
      "['method', 'risk', 'skill', 'learning', 'transversal skill', 'transversal', 'exercise', 'activity', 'optic', 'introduction']\n",
      "\n",
      "Topic 15: \u001b[1mMicroengineering\u001b[0m \n",
      "['system', 'sensor', 'application', 'material', 'design', 'technology', 'device', 'field', 'electronics', 'method']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the LDA model\n",
    "epfl_lda = LDA.train(term_doc_matrix, k=k, docConcentration=float(alpha), topicConcentration=float(beta), seed=0)\n",
    "\n",
    "# Infered EPFL taught subjects, written by hand after a first print_topics() call\n",
    "labels = [\n",
    "    \"Computer Science\",\n",
    "    \"Mathematics?\",\n",
    "    \"Biology/Life Sciences\",\n",
    "    \"Digital Humanities\",\n",
    "    \"Scientific Research\",\n",
    "    \"Quantum Sciences\",\n",
    "    \"Evaluation related stuff\",\n",
    "    \"Nuclear Physic\",\n",
    "    \"Electrical & Electronic Engineering\",\n",
    "    \"Electromagnetism\",\n",
    "    \"Chemistry\",\n",
    "    \"Data Science\",\n",
    "    \"Material Sciences\",\n",
    "    \"Finance Engineering\",\n",
    "    \"Microengineering\"\n",
    "]\n",
    "\n",
    "# Print the words for each topics\n",
    "print_topics(epfl_lda, labels=labels, words_per_topic=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.11: Wikipedia structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wikipedia RDD\n",
    "wikipedia_RDD = sc.textFile('/ix/wikipedia-for-schools.txt').map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the identifier for all wikipedia pages\n",
    "wikiPageIds_RDD = wikipedia_RDD.map(lambda p: p['page_id']).distinct()\n",
    "\n",
    "# The number of distinct pages\n",
    "N = wikiPageIds_RDD.count()\n",
    "\n",
    "# Map each page to a unique index \n",
    "pageToIdx = dict(zip(wikiPageIds_RDD.collect(), range(N)))\n",
    "pageIdx = list(pageToIdx.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all distinct terms from all wiki pages\n",
    "all_words_RDD = wikipedia_RDD.flatMap(lambda p: p[\"tokens\"]).distinct()\n",
    "\n",
    "# The number of distinct terms\n",
    "M = all_words_RDD.count() \n",
    "\n",
    "# Map each term to a unique index\n",
    "termToIdx = dict(zip(all_words_RDD.collect(), range(M)))\n",
    "vectorized_termToIdx = np.vectorize(lambda x: termToIdx[x])\n",
    "idxToTerm = {v: k for k, v in termToIdx.items()}\n",
    "\n",
    "# Compute a reduced version of the wikipedia RDD with only indexes\n",
    "red_wiki_RDD = wikipedia_RDD.map(lambda c: (pageToIdx[c[\"page_id\"]], vectorized_termToIdx(c[\"tokens\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  1: \u001b[1mOlympic Games\u001b[0m \n",
      "['games', 'game', 'players', 'world', 'time', 'olympic', '–', 'cup', 'player', 'events']\n",
      "\n",
      "Topic  2: \u001b[1mNumber Theory\u001b[0m \n",
      "['theory', 'number', '=', 'numbers', 'work', 'set', 'called', 'form', 'written', 'century']\n",
      "\n",
      "Topic  3: \u001b[1mUrban Development\u001b[0m \n",
      "['city', '·', 'centre', 'century', 'law', 'population', 'system', 'government', 'state', 'years']\n",
      "\n",
      "Topic  4: \u001b[1mHealthcare\u001b[0m \n",
      "['blood', 'people', 'health', 'cancer', 'medical', 'treatment', 'risk', 'patients', 'high', 'years']\n",
      "\n",
      "Topic  5: \u001b[1mVolcanic Eruptions\u001b[0m \n",
      "['eruption', 'years', 'comet', 'lava', 'volcanic', 'volcano', 'india', 'soil', 'large', 'time']\n",
      "\n",
      "Topic  6: \u001b[1mIsland Geography\u001b[0m \n",
      "['island', 'islands', 'european', 'city', 'population', 'country', 'north', 'south', 'ireland', 'east']\n",
      "\n",
      "Topic  7: \u001b[1mUrban Planning\u001b[0m \n",
      "['south', 'lake', 'mi', 'area', 'river', 'city', 'oil', 'water', 'population', 'north']\n",
      "\n",
      "Topic  8: \u001b[1mOptics\u001b[0m \n",
      "['gas', 'lens', 'game', 'time', 'earth', 'lenses', 'water', 'number', 'ds', 'haiku']\n",
      "\n",
      "Topic  9: \u001b[1mMusic and Art\u001b[0m \n",
      "['music', 'instruments', 'painting', 'art', 'made', 'popular', 'instrument', 'set', 'bass', 'style']\n",
      "\n",
      "Topic 10: \u001b[1mCalendar\u001b[0m \n",
      "['american', '–', 'calendar', 'january', 'march', 'february', 'july', 'april', 'june', 'december']\n",
      "\n",
      "Topic 11: \u001b[1mComputer Science\u001b[0m \n",
      "['computer', 'software', 'oil', 'language', 'acid', 'system', 'internet', 'apple', '^', 'version']\n",
      "\n",
      "Topic 12: \u001b[1mEnvironment\u001b[0m \n",
      "['energy', 'water', 'mass', 'light', 'earth', 'surface', 'chemical', 'form', 'temperature', 'solar']\n",
      "\n",
      "Topic 13: \u001b[1mEntertainment\u001b[0m \n",
      "['film', 'john', 'england', 'series', 'time', 'london', '–', 'years', 'house', 'george']\n",
      "\n",
      "Topic 14: \u001b[1mGovernment and Politics\u001b[0m \n",
      "['government', 'state', 'rights', 'states', 'china', 'united', 'president', 'national', 'city', 'republic']\n",
      "\n",
      "Topic 15: \u001b[1mRivers and Waterways\u001b[0m \n",
      "['river', 'sea', 'lake', 'area', 'north', 'world', 'large', 'south', 'years', 'al']\n",
      "\n",
      "Topic 16: \u001b[1mAncient History and Mythology\u001b[0m \n",
      "['bc', 'gods', 'egyptian', 'horse', 'god', 'egypt', 'mythology', 'modern', 'greek', 'temple']\n",
      "\n",
      "Topic 17: \u001b[1mWar and Conflict\u001b[0m \n",
      "['war', 'british', 'american', 'united', 'german', 'september', 'august', 'states', 'july', 'army']\n",
      "\n",
      "Topic 18: \u001b[1mSpace Exploration\u001b[0m \n",
      "['ice', 'space', 'war', 'nuclear', 'soviet', 'forces', 'weapons', 'time', 'force', 'mission']\n",
      "\n",
      "Topic 19: \u001b[1mInformation Systems\u001b[0m \n",
      "['system', 'systems', 'energy', 'information', 'distribution', 'data', 'number', 'dna', 'engine', 'process']\n",
      "\n",
      "Topic 20: \u001b[1mBusiness\u001b[0m \n",
      "['company', '$', 'war', 'market', 'states', 'world', 'united', 'government', 'system', 'japanese']\n",
      "\n",
      "Topic 21: \u001b[1mHumanity and Culture\u001b[0m \n",
      "['human', 'people', 'africa', 'years', 'world', 'life', 'india', 'family', 'early', 'time']\n",
      "\n",
      "Topic 22: \u001b[1mTea Culture\u001b[0m \n",
      "['tea', 'fruit', 'time', 'seeds', 'called', 'word', 'common', 'long', 'mario', 'leaves']\n",
      "\n",
      "Topic 23: \u001b[1mIndian History\u001b[0m \n",
      "['india', 'century', 'indian', 'sri', 'south', 'west', 'north', 'sheep', 'found', 'world']\n",
      "\n",
      "Topic 24: \u001b[1mFrench History\u001b[0m \n",
      "['king', 'war', 'church', 'french', '–', 'roman', 'century', 'empire', 'english', 'france']\n",
      "\n",
      "Topic 25: \u001b[1mBiodiversity\u001b[0m \n",
      "['species', 'water', 'plants', 'cells', 'food', 'disease', 'common', 'found', 'animals', 'plant']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Helper function to create the sparse vector of size M the number of terms for each page\n",
    "def create_sparse_vector_from_document(doc):\n",
    "    vector = {}\n",
    "    for termIdx in doc[1]:\n",
    "        vector[termIdx] = vector.get(termIdx, 0) + 1\n",
    "    return (doc[0], Vectors.sparse(M, vector))\n",
    "\n",
    "# Build the term-document matrix\n",
    "term_doc_matrix = red_wiki_RDD.map(lambda x: create_sparse_vector_from_document(x)).map(list)\n",
    "\n",
    "# Dirichlet hyperparameters\n",
    "k = 25\n",
    "alpha = 1.01\n",
    "beta = 1.5\n",
    "\n",
    "# Train the LDA model\n",
    "lda = LDA.train(term_doc_matrix, k=k, docConcentration=float(alpha), topicConcentration=float(beta), seed=0)\n",
    "\n",
    "# Function used to retrieve the words from their respective index\n",
    "def print_topics(model, idxToTerm=idxToTerm, labels=None, words_per_topic=10):\n",
    "    topics = model.describeTopics(words_per_topic)\n",
    "    for i, topic in enumerate(topics):\n",
    "        wordsIdx = topic[0]\n",
    "        words = []\n",
    "        for idx in wordsIdx:\n",
    "            word = idxToTerm[idx]\n",
    "            words.append(word)\n",
    "        if labels != None:\n",
    "            print(f'Topic {i + 1 :>2}: \\033[1m{labels[i]}\\033[0m \\n{words}\\n')\n",
    "        else: print(f'Topic {i + 1 :>2}: {words}')\n",
    "\n",
    "# Infered labels, written by hand after a first print_topics() call\n",
    "labels = ['Olympic Games', 'Number Theory', 'Urban Development', 'Healthcare', 'Volcanic Eruptions',\n",
    "          'Island Geography', 'Urban Planning', 'Optics', 'Music and Art', 'Calendar',\n",
    "          'Computer Science', 'Environment', 'Entertainment', 'Government and Politics',\n",
    "          'Rivers and Waterways', 'Ancient History and Mythology', 'War and Conflict', 'Space Exploration',\n",
    "          'Information Systems', 'Business', 'Humanity and Culture', 'Tea Culture', 'Indian History',\n",
    "          'French History', 'Biodiversity']\n",
    "\n",
    "# Finally print the topics\n",
    "print_topics(lda, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dirichlets hyperparameter choice explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of topics on Wikipedia is extensive, it is reasonable to expect that a larger value of $k$ would yield better results. However, due to memory limitations, we had to restrict ourselves to $k=25$. Regarding  $α$, since each Wikipedia page focuses on a highly specialized subject, the topics are less likely to overlap across pages. This is why we set $\\alpha=1.01$ that is to the minimum value. As for $\\beta$, considering the vast number of words in a subset of Wikipedia, it is inevitable that words will be repeated across various topics. Therefore, by selecting $\\beta=1.5$, we introduced some level of tolerance.  \n",
    "\n",
    "We observe that the topics are quite discernible, and thus, we are satisfied with the results achieved with $k=25$. However, it is important to acknowledge that there may be hundreds of additional topics beyond these."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
