{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 4: Word2Vec\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** K\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* Luca Mouchel\n",
    "* Mathieu Sauser\n",
    "* Heikel Jebali\n",
    "* Jérémy Chaverot\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 4 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import string\n",
    "\n",
    "from utils import *\n",
    "import gensim\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/ix/model.txt')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what kind of words are not represented in the model. Let's take the first description in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discussed.\n",
      "Nanocomposites,\n",
      "biocomposites\n",
      "presented.\n",
      "development,\n",
      "work.\n",
      "materialsConstituentsProcessing\n",
      "compositesDesign\n",
      "structures Current\n",
      "developmentNanocomposites\n",
      "compositesBiocompositesAdaptive\n",
      "composites ApplicationsDriving\n",
      "marketsCost\n",
      "analysisAerospaceAutomotiveSport\n",
      "Nanocomposites\n",
      "Biocomposites\n",
      "Prerequisites\n",
      "By\n",
      "course,\n",
      "to:\n",
      "Propose\n",
      "design,\n",
      "partApply\n",
      "materialsDiscuss\n",
      "task.Use\n",
      "IT\n",
      "toolsCommunicate\n",
      "disciplines.Evaluate\n",
      "one's\n",
      "team,\n",
      "feedback.\n",
      "part,\n",
      " \n"
     ]
    }
   ],
   "source": [
    "## Just print examples of words that are not in the model\n",
    "description = courses[0]['description']\n",
    "words = description.split(\" \")\n",
    "for word in words:\n",
    "    if word not in stopwords:\n",
    "        try:\n",
    "            model[word]\n",
    "        except KeyError:\n",
    "            print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some words are not separated by whitespace (e.g., _\"analysisAerospaceAutomotiveSport\"_) or some words have some punctuation attached to them (e.g., _\"part,\"_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Our preprocessing consists of separating strings like: abcDef into words: abc, Def \n",
    "It also fixes the isuse where the word 3D is appended to words so the model doesnt recognize the word properly\n",
    "Hence we separate 3D from the word theyre appended to (e.g., \"image3D\" becomes \"image\", \"3D\")\n",
    "the same problem occurs with Ph D.\n",
    "Then we also remove all punctuation because it is not necessary\n",
    "Finally we remove words who appear above the max_freq amount of times\n",
    "\"\"\"\n",
    "def preprocess(description, max_freq=25):\n",
    "    description = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', description) # this allows to separate words like lookingForAnswers into looking For Answers \n",
    "    description = re.sub(r'3D', r' 3D ', description) # there are several occasions where 3D is stuck besides a word like this 3DMovie or something\n",
    "    description = re.sub(r'Ph D', r'PhD', description) #the same thing happens with PhD\n",
    "    words = description.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))).split() \n",
    "    term_frequency = {}\n",
    "    for word in words:\n",
    "        if word in term_frequency:\n",
    "            term_frequency[word] += 1\n",
    "        else:\n",
    "            term_frequency[word] = 1\n",
    "            \n",
    "    frequent_words = set([word for word in words if term_frequency[word] >= max_freq])\n",
    "    if frequent_words != set():\n",
    "        print(frequent_words)\n",
    "\n",
    "    clean = [word for word in words if word.lower() not in stopwords and not word.isdigit() and term_frequency[word] < max_freq]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a **clean** dataset, where the description of courses are preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as you can see, the terms that appear frequently are terms that do not contribute to the meaning of the text and can therefore be removed\n",
      "{'and', 'the'}\n",
      "{'of', 'the', 'and'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'the'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'of'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'the'}\n",
      "{'of', 'and', 'the'}\n",
      "{'of'}\n",
      "{'the'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'of'}\n",
      "{'of', 'the'}\n",
      "{'and', 'the', 'to'}\n",
      "{'and'}\n",
      "{'and'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'and'}\n",
      "{'of', 'the'}\n",
      "{'of'}\n",
      "{'of', 'and', 'the'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'the'}\n",
      "{'and', 'the'}\n",
      "{'and', 'the'}\n",
      "{'of', 'and', 'the'}\n",
      "{'the'}\n",
      "{'and', 'the'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'of', 'and', 'the'}\n",
      "{'of', 'and', 'the'}\n",
      "{'the'}\n",
      "{'the', 'and'}\n",
      "{'and'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'of', 'the'}\n",
      "{'and'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'and'}\n",
      "{'of', 'the'}\n",
      "{'and', 'the'}\n",
      "{'and'}\n",
      "{'of'}\n",
      "{'the'}\n",
      "{'of'}\n",
      "{'the'}\n",
      "{'the'}\n",
      "{'the'}\n",
      "{'of'}\n",
      "{'and'}\n",
      "{'of', 'the'}\n",
      "{'and'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'and'}\n",
      "{'and'}\n",
      "{'and', 'the'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'and', 'the'}\n",
      "{'and'}\n",
      "{'of', 'and', 'the'}\n",
      "{'and'}\n",
      "{'and', 'the'}\n",
      "{'of', 'and', 'the'}\n",
      "{'of', 'and', 'the'}\n",
      "{'of', 'and'}\n",
      "{'and'}\n",
      "{'of', 'the'}\n",
      "{'of', 'a', 'the', 'to'}\n",
      "{'the'}\n",
      "{'the'}\n",
      "{'of', 'and', 'the'}\n",
      "{'and'}\n",
      "{'of', 'and', 'the'}\n",
      "{'of', 'and', 'the'}\n",
      "{'of', 'and', 'the', 'to'}\n",
      "{'of', 'and'}\n",
      "{'the'}\n",
      "{'of'}\n",
      "{'the'}\n",
      "{'the'}\n",
      "{'the'}\n",
      "{'of', 'and', 'the'}\n",
      "{'of'}\n",
      "{'the'}\n",
      "{'the'}\n",
      "{'the'}\n",
      "{'the'}\n",
      "{'of'}\n",
      "{'and'}\n",
      "{'the'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'of'}\n",
      "{'the'}\n",
      "{'and'}\n",
      "{'of', 'and', 'to'}\n",
      "{'and', 'the'}\n",
      "{'the'}\n"
     ]
    }
   ],
   "source": [
    "print(\"as you can see, the terms that appear frequently are terms that do not contribute to the meaning of the text and can therefore be removed\")\n",
    "clean_courses = list(map(lambda course: (course['courseId'], course['name'], preprocess(course['description'])), courses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.12 : Clustering word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vectors in the model have shape (300, 1) so we try to fetch words from the model and if the word isnt \n",
    "in the model, we simply return a 300 long vector of 0s\n",
    "\"\"\"\n",
    "def get_vector(word):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        return np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We collect all the words that have been preprocessed from the descriptions of each course\n",
    "all_words = [word for ls in list(map(lambda t : t[2], clean_courses)) for word in ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For KMeans, we discard words that are not in the model as this would create one large cluster for the default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(filter(lambda word: word in model.key_to_index, all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(map(lambda word: (word, get_vector(word)), all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the KMeans model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=30, random_state=0).fit(list(w2v.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = kmeans.predict(list(w2v.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 closest words to cluster 1 are:\n",
      "['Methodology' 'Paradigms' 'Methods' 'Perturbation' 'Perspective'\n",
      " 'Processes' 'Analysis' 'Methodologies' 'Theory' 'Context']\n",
      "\n",
      "The 10 closest words to cluster 2 are:\n",
      "['arguments' 'questions' 'explaining' 'facts' 'questioning' 'implication'\n",
      " 'justification' 'comment' 'arguing' 'statements']\n",
      "\n",
      "The 10 closest words to cluster 3 are:\n",
      "['renderings' 'compositions' 'pictorial' 'illustrative' 'pictures'\n",
      " 'images' 'drawings' 'illustrating' 'texts' 'illustrations']\n",
      "\n",
      "The 10 closest words to cluster 4 are:\n",
      "['equations' 'convolution' 'univariate' 'discretized' 'invariant'\n",
      " 'eigenvalues' 'summability' 'finite' 'ODEs' 'quadratic']\n",
      "\n",
      "The 10 closest words to cluster 5 are:\n",
      "['Dielectric' 'Reactivity' 'Waveform' 'Pulsed' 'Extrusion' 'Detectors'\n",
      " 'Impedance' 'Measurement' 'Diode' 'Resistive']\n",
      "\n",
      "The 10 closest words to cluster 6 are:\n",
      "['graduates' 'education' 'taught' 'faculty' 'students' 'coursework'\n",
      " 'courses' 'graduate' 'teaching' 'undergraduate']\n",
      "\n",
      "The 10 closest words to cluster 7 are:\n",
      "['transduction' 'tyrosine' 'apoptosis' 'DNase' 'phosphorylation'\n",
      " 'extracellular' 'enzymatic' 'protein' 'intracellular' 'proteins']\n",
      "\n",
      "The 10 closest words to cluster 8 are:\n",
      "['informs' 'learn' 'understands' 'realise' 'letting' 'discover'\n",
      " 'realizing' 'find' 'realize' 'knowing']\n",
      "\n",
      "The 10 closest words to cluster 9 are:\n",
      "['equity' 'securities' 'suppliers' 'firms' 'cashflow' 'pricing'\n",
      " 'investors' 'investing' 'investment' 'investments']\n",
      "\n",
      "The 10 closest words to cluster 10 are:\n",
      "['precisely' 'derive' 'differ' 'specific' 'contexts' 'instance'\n",
      " 'differentiated' 'distinct' 'characteristic' 'analogous']\n",
      "\n",
      "The 10 closest words to cluster 11 are:\n",
      "['photons' 'excitation' 'thermodynamical' 'photocurrent' 'vibronic'\n",
      " 'anisotropy' 'photoionization' 'nonlinearities' 'photoluminescence'\n",
      " 'photoemission']\n",
      "\n",
      "The 10 closest words to cluster 12 are:\n",
      "['aquifer' 'sedimentation' 'evaporation' 'wastewaters' 'biomass' 'soils'\n",
      " 'vegetation' 'contaminants' 'sediment' 'groundwater']\n",
      "\n",
      "The 10 closest words to cluster 13 are:\n",
      "['evaluate' 'methodologies' 'efficient' 'assessing' 'optimized' 'optimal'\n",
      " 'optimisation' 'utilization' 'evaluating' 'optimizing']\n",
      "\n",
      "The 10 closest words to cluster 14 are:\n",
      "['Philipp' 'Emil' 'Moser' 'Meyer' 'Schmid' 'Bernhard' 'Hans' 'Georg'\n",
      " 'Schürmann' 'Werner']\n",
      "\n",
      "The 10 closest words to cluster 15 are:\n",
      "['epilepsy' 'cardiovascular' 'clinically' 'disorders' 'neuroprotection'\n",
      " 'dysfunction' 'inflammation' 'tumors' 'neurologic' 'neurological']\n",
      "\n",
      "The 10 closest words to cluster 16 are:\n",
      "['Librairie' 'toute' 'les' 'mémoire' 'dans' 'édition' 'Mathématiques'\n",
      " 'été' 'illustré' 'chimie']\n",
      "\n",
      "The 10 closest words to cluster 17 are:\n",
      "['photodiodes' 'Capacitive' 'Piezoelectric' 'optical' 'transducers'\n",
      " 'actuators' 'circuitry' 'sensors' 'photomultipliers' 'photodetectors']\n",
      "\n",
      "The 10 closest words to cluster 18 are:\n",
      "['Living' 'Imagination' 'Trouble' 'Noise' 'Face' 'Hands' 'Perfect' 'Shape'\n",
      " 'Mind' 'Make']\n",
      "\n",
      "The 10 closest words to cluster 19 are:\n",
      "['mesh' 'curved' 'surfaces' 'thick' 'concave' 'casing' 'horizontal'\n",
      " 'vertical' 'cylindrical' 'thin']\n",
      "\n",
      "The 10 closest words to cluster 20 are:\n",
      "['Simulink' 'interface' 'functionality' 'Javascript' 'GUI' 'interfaces'\n",
      " 'frontend' 'implementations' 'APIs' 'software']\n",
      "\n",
      "The 10 closest words to cluster 21 are:\n",
      "['depletion' 'accumulation' 'increasing' 'irreversible' 'fluctuation'\n",
      " 'resulting' 'increases' 'disruption' 'instability' 'fluctuations']\n",
      "\n",
      "The 10 closest words to cluster 22 are:\n",
      "['Strategy' 'Sustainable' 'Information' 'Policy' 'Infrastructure'\n",
      " 'Evaluation' 'Innovation' 'Management' 'Implementation' 'Resource']\n",
      "\n",
      "The 10 closest words to cluster 23 are:\n",
      "['perspectives' 'emphasizing' 'context' 'rationality' 'understanding'\n",
      " 'subjectivity' 'notion' 'worldview' 'notions' 'concepts']\n",
      "\n",
      "The 10 closest words to cluster 24 are:\n",
      "['Immunology' 'Physics' 'Genetics' 'Molecular' 'Computational'\n",
      " 'Biophysics' 'Biomedical' 'Biochemistry' 'Neuroscience' 'Microbiology']\n",
      "\n",
      "The 10 closest words to cluster 25 are:\n",
      "['August' 'September' 'time' 'week' 'scoring' 'ended' 'start' 'final'\n",
      " 'consecutive' 'starting']\n",
      "\n",
      "The 10 closest words to cluster 26 are:\n",
      "['hydrolysis' 'nitrides' 'compounds' 'polymeric' 'thiols' 'silane'\n",
      " 'polymers' 'polyol' 'azides' 'carbonylation']\n",
      "\n",
      "The 10 closest words to cluster 27 are:\n",
      "['Freeman' 'Robinson' 'Allen' 'Richardson' 'Taylor' 'Patterson' 'Baker'\n",
      " 'Smith' 'Moore' 'Thompson']\n",
      "\n",
      "The 10 closest words to cluster 28 are:\n",
      "['representatives' 'cooperation' 'leadership' 'constitutional' 'policy'\n",
      " 'establishment' 'governance' 'policies' 'government' 'governmental']\n",
      "\n",
      "The 10 closest words to cluster 29 are:\n",
      "['ARCH' 'FLASH' 'MAP' 'VQ' 'PTM' 'IC' 'EM' 'MGT' 'ENV' 'EER']\n",
      "\n",
      "The 10 closest words to cluster 30 are:\n",
      "['sited' 'vicinity' 'stretching' 'connect' 'area' 'linking' 'situated'\n",
      " 'junction' 'adjacent' 'connecting']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here we just print the top ten closest words to each cluster.\n",
    "\"\"\"\n",
    "for i, center in enumerate(kmeans.cluster_centers_):\n",
    "    similarities = model.cosine_similarities(center, np.array(list(w2v.values()))[clusters == i])\n",
    "    top_idx = np.argsort(similarities)[-10:]\n",
    "    print(f'The 10 closest words to cluster {i+1} are:\\n{np.array(list(w2v.keys()))[clusters == i][top_idx]}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that clusters are somewhat related to science, concepts (understanding, time), names or course topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling clusters\n",
    "* cluster #1: Methodology\n",
    "* cluster #3: Illustrations and pictures\n",
    "* cluster #4: Mathematics\n",
    "* cluster #6: Education and School work\n",
    "* cluster #7: biology and cell vocabulary\n",
    "* cluster #9: Finance and investment\n",
    "* cluster #15: Medicine and medical conditions\n",
    "* cluster #20: Programming and software\n",
    "* cluster #24: Biology, physics and chemistry\n",
    "* cluster #25: notion of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with LSI and LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster #7, #4, #1 and other clusters are very similar to those found using LDA. Similarly, clusters #4 is similar here and in LSI. Otherwise, words tend to be more closely tied to their clusters in this case. For example for mathematics: LDA finds: 'learning', 'model', 'time', 'method', 'management', 'analysis', 'stochastic' whereas we find: 'equations', 'convolution', 'univariate', 'discretized', 'invariant',\n",
    " 'eigenvalues', 'summability', 'finite', 'ODEs', and 'quadratic'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.13 : Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = {}\n",
    "for word in all_words:\n",
    "    if word not in DF:\n",
    "        DF[word] = 1\n",
    "    else: DF[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def get_DF(word):\n",
    "    if word not in model:\n",
    "        DF[word] = 1\n",
    "    return DF[word]\n",
    "\n",
    "\n",
    "def compute_TF_IDF(description):\n",
    "    TF_IDF = {word: TF/get_DF(word) for word, TF in Counter(description).items()}\n",
    "    total_TF_IDF = np.sum(list(TF_IDF.values()))\n",
    "    vec = np.sum([get_vector(word) * TF_IDF[word] / total_TF_IDF  for word in description], axis=0)\n",
    "    return TF_IDF, total_TF_IDF, vec\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Basically, here we compute the TF-IDF for a course which has been preprocessed (clean_course) and we also take the index\n",
    "of the course as a parameter to provide the course description of the original data.\n",
    "The dataset is transformed into : {course_id, name, description, vector}\n",
    "vector is the addition to the data. It is calculated by a weighted average of word vectors based on the TF-IDF scores of the words in the course's description. \n",
    "It combines the word vectors by multiplying each vector by its respective TF-IDF weight and then normalizing the result by dividing by the total TF-IDF score. \n",
    "Finally, the vectors are summed to obtain a single vector representation for the course's description.\n",
    "\"\"\"\n",
    "def transform_course_description_to_vector(i, clean_course):\n",
    "    description = clean_course[2]\n",
    "    TF_IDF, total_TF_IDF, vec = compute_TF_IDF(description)\n",
    "    return {'courseId': clean_course[0],\n",
    "            'name': clean_course[1],\n",
    "            'description': courses[i]['description'],\n",
    "            'vector': vec}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Get a vector representation for courses and their descriptions\n",
    "\"\"\"\n",
    "def vectorize_course_descriptions():\n",
    "    return list(map(lambda index_to_course: transform_course_description_to_vector(index_to_course[0], index_to_course[1]), enumerate(clean_courses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contained_in_DF(words):\n",
    "    for word in words:\n",
    "        if word not in DF:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def get_search_vector(query):\n",
    "    words = query.split(\" \")\n",
    "    if contained_in_DF(words):\n",
    "        _, _, vec = compute_TF_IDF(words)\n",
    "        return vec\n",
    "    else:\n",
    "        return np.mean([get_vector(word) for word in words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_with_vectors = vectorize_course_descriptions()\n",
    "def get_top_courses(query_vector,top=5):\n",
    "    top_matches = list(map(lambda course: (course, model.cosine_similarities(query_vector, [course['vector']])[0]), courses_with_vectors))\n",
    "    top_matches.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    top_matches = top_matches[:top]\n",
    "    return top_matches\n",
    "\n",
    "def print_top_matching_courses(query):\n",
    "    search_vector = get_search_vector(query)\n",
    "    top_matches = get_top_courses(search_vector)\n",
    "    i = 1\n",
    "    for course in top_matches:\n",
    "        print(\"Result {i}: course {courseId}, {name}, with similarity: {dist}\".format(i=i, courseId=course[0]['courseId'], name=course[0]['name'], dist=round(course[1],3)))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: course MATH-332, Applied stochastic processes, with similarity: 0.562\n",
      "Result 2: course MGT-484, Applied probability & stochastic processes, with similarity: 0.529\n",
      "Result 3: course COM-516, Markov chains and algorithmic applications, with similarity: 0.519\n",
      "Result 4: course CH-311, Molecular and cellular biophysic I, with similarity: 0.483\n",
      "Result 5: course MSE-211, Organic chemistry, with similarity: 0.472\n"
     ]
    }
   ],
   "source": [
    "print_top_matching_courses(\"Markov chains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: course EE-727, Computational Social Media, with similarity: 0.755\n",
      "Result 2: course COM-308, Internet analytics, with similarity: 0.494\n",
      "Result 3: course CS-622, Privacy Protection, with similarity: 0.478\n",
      "Result 4: course COM-208, Computer networks, with similarity: 0.472\n",
      "Result 5: course CS-486, Human computer interaction, with similarity: 0.469\n"
     ]
    }
   ],
   "source": [
    "print_top_matching_courses(\"Facebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with the vector space model and LSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing with the vector space model we find that **MATH-332, COM-516, MGT-484** are in the top 5 for both W2V and the VSM. Likewise, LSI and W2V find the same relevant courses. All of LSI, W2V and LDA yield similar results. However, w2v gives 2 chemistry courses which are not relevant to markov chains. Nevertheless, the results are quite good for both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.14: Document similarity search with outside terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: course EE-727, Computational Social Media, with similarity: 0.712\n",
      "Result 2: course COM-208, Computer networks, with similarity: 0.522\n",
      "Result 3: course COM-308, Internet analytics, with similarity: 0.519\n",
      "Result 4: course MGT-517, Entrepreneurship laboratory (e-lab), with similarity: 0.489\n",
      "Result 5: course CS-486, Human computer interaction, with similarity: 0.482\n"
     ]
    }
   ],
   "source": [
    "print_top_matching_courses(\"MySpace Orkut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting matching courses are similar to the ones when searching _Facebook_. This makes sense as _Orkut_ and _MySpace_ are two older social media platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: course BIO-657, Landmark Papers in Cancer and Infection, with similarity: 0.597\n",
      "Result 2: course BIO-477, Infection biology, with similarity: 0.588\n",
      "Result 3: course BIO-638, Practical - Lemaitre Lab, with similarity: 0.571\n",
      "Result 4: course CH-414, Pharmacological chemistry, with similarity: 0.548\n",
      "Result 5: course BIOENG-433, Biotechnology lab (for CGC), with similarity: 0.541\n"
     ]
    }
   ],
   "source": [
    "print_top_matching_courses(\"coronavirus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results seem somewhat reasonable, they are related to infections and biology, two topics closely related to coronavirus, given it is an infectious disease."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
